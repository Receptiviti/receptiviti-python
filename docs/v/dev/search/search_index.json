{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>A Python package to process text with the Receptiviti API.</p> <p>An R package is also available at Receptiviti/receptiviti-r.</p>"},{"location":"#installation","title":"Installation","text":"<p>If needed, download Python from python.org, then install the package with pip:</p> <p>Release (version 0.1.2)</p> <pre><code>pip install receptiviti\n</code></pre> <p>Development</p> <pre><code>pip install git+https://github.com/receptiviti/receptiviti-python.git\n</code></pre> <p>And load the package in a Python console:</p> <pre><code>import receptiviti\n</code></pre>"},{"location":"#examples","title":"Examples","text":"<pre><code># score a single text\nsingle = receptiviti.request(\"a text to score\")\n\n# score multiple texts, and write results to a file\nmulti = receptiviti.request([\"first text to score\", \"second text\"], \"filename.csv\")\n\n# score texts in separate files\n## defaults to look for .txt files\nfile_results = receptiviti.request(directory = \"./path/to/txt_folder\")\n\n## could be .csv\nfile_results = receptiviti.request(\n  directory = \"./path/to/csv_folder\",\n  text_column = \"text\", file_type = \"csv\"\n)\n\n# score texts in a single file\nresults = receptiviti.request(\"./path/to/file.csv\", text_column = \"text\")\n</code></pre>"},{"location":"#api-access","title":"API Access","text":"<p>To access the API, you will need to load your key and secret, as found on your dashboard.</p> <p>You can enter these as arguments in each function call, but by default they will be looked for in these environment variables:</p> <pre><code>RECEPTIVITI_KEY=\"32lettersandnumbers\"\nRECEPTIVITI_SECRET=\"56LettersAndNumbers\"\n</code></pre> <p>You can store these in a <code>.env</code> file (in the current directory or <code>~/Documents</code>) permanently, or set them temporarily:</p> <pre><code>import os\nos.environ[\"RECEPTIVITI_KEY\"]=\"32lettersandnumbers\"\nos.environ[\"RECEPTIVITI_SECRET\"]=\"56LettersAndNumbers\"\n</code></pre>"},{"location":"CHANGELOG/","title":"Changelog","text":""},{"location":"CHANGELOG/#receptiviti-013","title":"receptiviti 0.1.3","text":""},{"location":"CHANGELOG/#additions","title":"Additions","text":"<ul> <li>Adds <code>collect_results</code> option for cache-only output.</li> <li>Adds framework checking and listing functionality.</li> <li>Adds custom norming context creation functionality.</li> <li>Adds support for V2 of the API.</li> </ul>"},{"location":"CHANGELOG/#improvements","title":"Improvements","text":"<ul> <li>Makes <code>pyarrow</code> optional.</li> <li>Improves cache performance.</li> <li>Validates <code>version</code> and <code>endpoint</code>.</li> </ul>"},{"location":"CHANGELOG/#bug-fixes","title":"Bug Fixes","text":"<ul> <li>Avoids skipping a row when splitting oversized bundles.</li> <li>Avoids overwriting existing cache results within overlapping bins on update.</li> </ul>"},{"location":"CHANGELOG/#receptiviti-012","title":"receptiviti 0.1.2","text":""},{"location":"CHANGELOG/#improvements_1","title":"Improvements","text":"<ul> <li>Changes default number of cores to 1, to avoid unexpected behavior when running from a script.</li> <li>Improves environment file resolution.</li> </ul>"},{"location":"CHANGELOG/#bug-fixes_1","title":"Bug Fixes","text":"<ul> <li>Corrects order of output when reading from a file and <code>ids</code> are not specified.</li> <li>Fixes detection of some file encodings.</li> <li>Avoids issues when <code>receptiviti.request</code> is called from a script and is processing in parallel.</li> </ul>"},{"location":"CHANGELOG/#receptiviti-011","title":"receptiviti 0.1.1","text":""},{"location":"CHANGELOG/#improvements_2","title":"Improvements","text":"<ul> <li>Adds <code>encoding</code> argument; improves handling of non-UTF-8 files.</li> </ul>"},{"location":"CHANGELOG/#bug-fixes_2","title":"Bug Fixes","text":"<ul> <li>Fixes reading in files when <code>collapse_line</code> is <code>True</code>.</li> </ul>"},{"location":"CHANGELOG/#receptiviti-010","title":"receptiviti 0.1.0","text":"<p>First release.</p>"},{"location":"articles/commencement_example/","title":"Commencement example","text":"<p>This example uses the <code>Receptiviti</code> API to analyze commencement speeches.</p> In\u00a0[2]: Copied! <pre>import pandas\n\nspeeches = pandas.read_csv(\n  \"https://raw.githubusercontent.com/whatrocks/markov-commencement-speech\"\n  \"/refs/heads/master/speech_metadata.csv\"\n)\n\nspeeches.iloc[0:5, 1:4]\n</pre> import pandas  speeches = pandas.read_csv(   \"https://raw.githubusercontent.com/whatrocks/markov-commencement-speech\"   \"/refs/heads/master/speech_metadata.csv\" )  speeches.iloc[0:5, 1:4] Out[2]: name school year 0 Aaron Sorkin Syracuse University 2012 1 Abigail Washburn Colorado College 2012 2 Adam Savage Sarah Lawrence College 2012 3 Adrienne Rich Douglass College 1977 4 Ahmed Zewail Caltech 2011 <p>One file in the source repository contains an invalid character on Windows (<code>:</code>), so we'll need to pull them in individually, rather than cloning the repository:</p> In\u00a0[3]: Copied! <pre>import os\nimport requests\n\ntext_dir = \"../../../commencement_speeches/\"\nos.makedirs(text_dir, exist_ok=True)\n\ntext_url = (\n  \"https://raw.githubusercontent.com/whatrocks/commencement-db\"\n  \"/refs/heads/master/src/pages/\"\n)\nfor file in speeches[\"filename\"]:\n    out_file = text_dir + file.replace(':', '')\n    if not os.path.isfile(out_file):\n        req = requests.get(\n            text_url + file.replace(\".txt\", \"/index.md\"), timeout=999\n        )\n        if req.status_code != 200:\n            print(f\"failed to retrieve {file}\")\n            continue\n        with open(out_file, \"w\", encoding=\"utf-8\") as content:\n            content.write(\"\\n\".join(req.text.split(\"\\n---\")[1:]))\n</pre> import os import requests  text_dir = \"../../../commencement_speeches/\" os.makedirs(text_dir, exist_ok=True)  text_url = (   \"https://raw.githubusercontent.com/whatrocks/commencement-db\"   \"/refs/heads/master/src/pages/\" ) for file in speeches[\"filename\"]:     out_file = text_dir + file.replace(':', '')     if not os.path.isfile(out_file):         req = requests.get(             text_url + file.replace(\".txt\", \"/index.md\"), timeout=999         )         if req.status_code != 200:             print(f\"failed to retrieve {file}\")             continue         with open(out_file, \"w\", encoding=\"utf-8\") as content:             content.write(\"\\n\".join(req.text.split(\"\\n---\")[1:])) In\u00a0[4]: Copied! <pre>import re\n\nbullet_pattern = re.compile(\"([a-z]) \u2022\")\n\n\ndef read_text(file: str):\n  with open(text_dir + file.replace(\":\", \"\"), encoding=\"utf-8\") as content:\n      text = bullet_pattern.sub(\"\\\\1. \", content.read())\n  return text\n\n\nspeeches[\"text\"] = [read_text(file) for file in speeches[\"filename\"]]\n</pre> import re  bullet_pattern = re.compile(\"([a-z]) \u2022\")   def read_text(file: str):   with open(text_dir + file.replace(\":\", \"\"), encoding=\"utf-8\") as content:       text = bullet_pattern.sub(\"\\\\1. \", content.read())   return text   speeches[\"text\"] = [read_text(file) for file in speeches[\"filename\"]] In\u00a0[5]: Copied! <pre>import receptiviti\n</pre> import receptiviti In\u00a0[6]: Copied! <pre># since our texts are from speeches,\n# it might make sense to use the spoken norming context\nprocessed = receptiviti.request(\n    speeches[\"text\"], version = \"v2\", context = \"spoken\"\n)\nprocessed = pandas.concat([speeches.iloc[:, 0:4], processed], axis=1)\n\nprocessed.iloc[0:5, 7:]\n</pre> # since our texts are from speeches, # it might make sense to use the spoken norming context processed = receptiviti.request(     speeches[\"text\"], version = \"v2\", context = \"spoken\" ) processed = pandas.concat([speeches.iloc[:, 0:4], processed], axis=1)  processed.iloc[0:5, 7:] Out[6]: summary.words_per_sentence summary.sentence_count summary.six_plus_words summary.capitals summary.emojis summary.emoticons summary.hashtags summary.urls big_5.extraversion big_5.active ... disc_dimensions.people_relationship_emotion_focus disc_dimensions.task_system_object_focus disc_dimensions.d_axis disc_dimensions.i_axis disc_dimensions.s_axis disc_dimensions.c_axis disc_dimensions.d_axis_proportional disc_dimensions.i_axis_proportional disc_dimensions.s_axis_proportional disc_dimensions.c_axis_proportional 0 19.522388 134 0.192278 0.029845 0 0 0.000000 0 64.362931 58.626324 ... 68.785932 48.847076 52.559226 62.370538 54.667259 46.067726 0.243708 0.289201 0.253483 0.213608 1 28.587156 109 0.246149 0.031928 0 0 0.000963 0 55.125763 61.658720 ... 58.656537 40.455881 45.894668 55.262383 53.025679 44.037115 0.231534 0.278793 0.267509 0.222163 2 12.072000 125 0.221339 0.034079 0 0 0.000000 0 58.409255 51.114651 ... 62.707516 52.665746 57.492323 62.734369 48.323395 44.285521 0.270125 0.294755 0.227046 0.208074 3 32.389831 59 0.311879 0.011606 0 0 0.000000 0 59.149444 29.627236 ... 68.591156 45.065873 46.459670 57.317375 59.781553 48.457056 0.219133 0.270345 0.281968 0.228554 4 24.326733 101 0.326007 0.023517 0 0 0.000000 0 72.021225 50.677059 ... 61.249178 55.538689 62.837175 65.988609 42.076374 40.066922 0.297850 0.312788 0.199443 0.189918 <p>5 rows \u00d7 216 columns</p> In\u00a0[7]: Copied! <pre>lsm_categories = [\n    \"liwc15.\" + c\n    for c in [\n        \"personal_pronouns\",\n        \"impersonal_pronouns\",\n        \"articles\",\n        \"auxiliary_verbs\",\n        \"adverbs\",\n        \"prepositions\",\n        \"conjunctions\",\n        \"negations\",\n        \"quantifiers\",\n    ]\n]\n\ncategory_means = processed[lsm_categories].agg(\"mean\")\nprocessed[\"lsm_mean\"] = (\n    1\n    - abs(processed[lsm_categories] - category_means)\n    / (processed[lsm_categories] + category_means)\n).agg(\"mean\", axis=1)\n\nprocessed.sort_values(\"lsm_mean\").iloc[0:10][\n    [\"name\", \"school\", \"year\", \"lsm_mean\", \"summary.word_count\"]\n]\n</pre> lsm_categories = [     \"liwc15.\" + c     for c in [         \"personal_pronouns\",         \"impersonal_pronouns\",         \"articles\",         \"auxiliary_verbs\",         \"adverbs\",         \"prepositions\",         \"conjunctions\",         \"negations\",         \"quantifiers\",     ] ]  category_means = processed[lsm_categories].agg(\"mean\") processed[\"lsm_mean\"] = (     1     - abs(processed[lsm_categories] - category_means)     / (processed[lsm_categories] + category_means) ).agg(\"mean\", axis=1)  processed.sort_values(\"lsm_mean\").iloc[0:10][     [\"name\", \"school\", \"year\", \"lsm_mean\", \"summary.word_count\"] ] Out[7]: name school year lsm_mean summary.word_count 99 Gary Malkowski Gallaudet University 2011 0.751292 2154 268 Theodor \u2018Dr. Seuss\u2019 Geisel Lake Forest College 1977 0.763584 96 178 Makoto Fujimura Belhaven University 2011 0.820082 2569 81 Dwight Eisenhower Penn State 1955 0.831307 2518 100 George C. Marshall Harvard University 1947 0.833347 1449 170 Lewis Lapham St. John\u2019s College 2003 0.834804 3688 232 Rev. Joseph L. Levesque Niagara University 2007 0.845988 483 86 Edward W. Brooke Wellesley College 1969 0.847166 3083 117 Janet Napolitano Northeastern University 2014 0.853360 1526 289 Whoopi Goldberg Savannah College of Art and Design 2011 0.855008 1248 <p>Here, it is notable that the most stylistically unique speech was delivered in American Sign Language, and the second most stylistically unique speech was a short rhyme.</p> <p>We might also want to see which speeches are most similar to one another:</p> In\u00a0[8]: Copied! <pre>import numpy\n\n# calculate all pairwise comparisons\nlsm_pairs = processed[lsm_categories].T.corr(\n    lambda a, b: numpy.mean(1 - abs(a - b) / (a + b))\n)\n\n# set self-matches to 0\nnumpy.fill_diagonal(lsm_pairs.values, 0)\n\n# identify the closes match to each speech\nspeeches[\"match\"] = lsm_pairs.idxmax()\nbest_match = lsm_pairs.max()\n\n# loo at the top matches\ntop_matches = best_match.sort_values(ascending=False).index[:20].astype(int).to_list()\ntop_match_pairs = pandas.DataFrame(\n    {\"a\": top_matches, \"b\": speeches[\"match\"][top_matches].to_list()}\n)\ntop_match_pairs = top_match_pairs[\n    ~top_match_pairs.apply(\n        lambda r: \"\".join(r.sort_values().astype(str)), 1\n    ).duplicated()\n]\npandas.concat(\n    [\n        speeches.iloc[top_match_pairs[\"a\"], 1:4].reset_index(drop=True),\n        pandas.DataFrame({\"Similarity\": best_match[top_match_pairs[\"a\"]]}).reset_index(\n            drop=True\n        ),\n        speeches.iloc[top_match_pairs[\"b\"], 1:4].reset_index(drop=True),\n    ],\n    axis=1,\n)\n</pre> import numpy  # calculate all pairwise comparisons lsm_pairs = processed[lsm_categories].T.corr(     lambda a, b: numpy.mean(1 - abs(a - b) / (a + b)) )  # set self-matches to 0 numpy.fill_diagonal(lsm_pairs.values, 0)  # identify the closes match to each speech speeches[\"match\"] = lsm_pairs.idxmax() best_match = lsm_pairs.max()  # loo at the top matches top_matches = best_match.sort_values(ascending=False).index[:20].astype(int).to_list() top_match_pairs = pandas.DataFrame(     {\"a\": top_matches, \"b\": speeches[\"match\"][top_matches].to_list()} ) top_match_pairs = top_match_pairs[     ~top_match_pairs.apply(         lambda r: \"\".join(r.sort_values().astype(str)), 1     ).duplicated() ] pandas.concat(     [         speeches.iloc[top_match_pairs[\"a\"], 1:4].reset_index(drop=True),         pandas.DataFrame({\"Similarity\": best_match[top_match_pairs[\"a\"]]}).reset_index(             drop=True         ),         speeches.iloc[top_match_pairs[\"b\"], 1:4].reset_index(drop=True),     ],     axis=1, ) Out[8]: name school year Similarity name school year 0 Cynthia Enloe Connecticut College 2011 0.999325 Howard Gordon Connecticut College 2013 1 Benjamin Carson Jr. Niagara University 2003 0.984177 Jonathon Youshaei Deerfield High School 2009 2 John Legend University of Pennsylvania 2014 0.980730 Sheryl Sandberg City Colleges of Chicago 2014 3 Ronald Reagan Eureka College 1957 0.979818 Arianna Huffington Sarah Lawrence College 2011 4 Amy Poehler Harvard University 2011 0.977867 Sheryl Sandberg City Colleges of Chicago 2014 5 Melissa Harris-Perry Wellesley College 2012 0.975869 Drew Houston Massachusetts Institute of Technology 2013 6 Alan Alda Connecticut College 1980 0.975361 Nora Ephron Wellesley College 1996 7 Woody Hayes Ohio State University 1986 0.975117 James Carville Hobart and William Smith Colleges 2013 8 Mindy Kaling Harvard Law School 2014 0.975033 Stephen Colbert Wake Forest University 2015 9 Tim Cook Auburn University 2010 0.975007 Arianna Huffington Vassar College 2015 10 Barbara Bush Wellesley College 1990 0.974678 Daniel S. Goldin Massachusetts Institute of Technology 2001 In\u00a0[9]: Copied! <pre>from statistics import linear_regression\nfrom matplotlib.pyplot import subplots\nfrom matplotlib.style import use\n\ndrive_data = processed.filter(regex=\"year|drives\")[processed[\"year\"] &gt; 1980]\ntrending_drives = (\n    drive_data.corrwith(drive_data[\"year\"])\n    .abs()\n    .sort_values(ascending=False)[1:4]\n    .index\n)\n\nfirst_year = drive_data[\"year\"].min()\ncolors = [\"#82C473\", \"#A378C0\", \"#616161\", \"#9F5C61\", \"#D3D280\"]\nlinestyles = [\"-\", \"--\", \":\", \"-.\", (5, (8, 2))]\nuse([\"dark_background\", {\"figure.facecolor\": \"#1e2229\", \"axes.facecolor\": \"#1e2229\"}])\nfig, ax = subplots()\nax.set(ylabel=\"Score\", xlabel=\"Year\")\nfor i, cat in enumerate(trending_drives):\n    points = ax.scatter(drive_data[\"year\"], drive_data[cat], color=colors[i])\n    beta, intercept = linear_regression(drive_data[\"year\"], drive_data[cat])\n    line = ax.axline(\n        (first_year, intercept + beta * first_year),\n        slope=beta,\n        color=colors[i],\n        linestyle=linestyles[i],\n        label=cat,\n    )\nlegend = ax.legend(loc=\"upper center\")\n</pre> from statistics import linear_regression from matplotlib.pyplot import subplots from matplotlib.style import use  drive_data = processed.filter(regex=\"year|drives\")[processed[\"year\"] &gt; 1980] trending_drives = (     drive_data.corrwith(drive_data[\"year\"])     .abs()     .sort_values(ascending=False)[1:4]     .index )  first_year = drive_data[\"year\"].min() colors = [\"#82C473\", \"#A378C0\", \"#616161\", \"#9F5C61\", \"#D3D280\"] linestyles = [\"-\", \"--\", \":\", \"-.\", (5, (8, 2))] use([\"dark_background\", {\"figure.facecolor\": \"#1e2229\", \"axes.facecolor\": \"#1e2229\"}]) fig, ax = subplots() ax.set(ylabel=\"Score\", xlabel=\"Year\") for i, cat in enumerate(trending_drives):     points = ax.scatter(drive_data[\"year\"], drive_data[cat], color=colors[i])     beta, intercept = linear_regression(drive_data[\"year\"], drive_data[cat])     line = ax.axline(         (first_year, intercept + beta * first_year),         slope=beta,         color=colors[i],         linestyle=linestyles[i],         label=cat,     ) legend = ax.legend(loc=\"upper center\") <p>To better visualize the effects, we might look between aggregated blocks of time:</p> In\u00a0[10]: Copied! <pre>summary = processed[trending_drives].aggregate([\"mean\", \"std\"])\nstandardized = ((processed[trending_drives] - summary.loc[\"mean\"])) / summary.loc[\"std\"]\ntime_median = int(processed[\"year\"].median())\nstandardized[\"Time Period\"] = pandas.Categorical(\n    processed[\"year\"] &gt;= time_median\n).set_categories([f\"&lt; {time_median}\", f\"&gt;= {time_median}\"], rename=True)\n\nsummaries = standardized.groupby(\"Time Period\", observed=True)[trending_drives].agg(\n    [\"mean\", \"size\"]\n)\n\nfig, ax = subplots()\nax.set(ylabel=\"Score (Scaled)\", xlabel=\"Time Period\")\nfor i, cat in enumerate(trending_drives):\n    summary = summaries[cat]\n    ax.errorbar(\n        summary.index,\n        summary[\"mean\"],\n        yerr=1 / summary[\"size\"] ** 0.5,\n        color=colors[i],\n        linestyle=linestyles[i],\n        label=cat,\n        capsize=6\n    )\nlegend = ax.legend(loc=\"upper center\")\n</pre> summary = processed[trending_drives].aggregate([\"mean\", \"std\"]) standardized = ((processed[trending_drives] - summary.loc[\"mean\"])) / summary.loc[\"std\"] time_median = int(processed[\"year\"].median()) standardized[\"Time Period\"] = pandas.Categorical(     processed[\"year\"] &gt;= time_median ).set_categories([f\"&lt; {time_median}\", f\"&gt;= {time_median}\"], rename=True)  summaries = standardized.groupby(\"Time Period\", observed=True)[trending_drives].agg(     [\"mean\", \"size\"] )  fig, ax = subplots() ax.set(ylabel=\"Score (Scaled)\", xlabel=\"Time Period\") for i, cat in enumerate(trending_drives):     summary = summaries[cat]     ax.errorbar(         summary.index,         summary[\"mean\"],         yerr=1 / summary[\"size\"] ** 0.5,         color=colors[i],         linestyle=linestyles[i],         label=cat,         capsize=6     ) legend = ax.legend(loc=\"upper center\") <p>This suggests that references to risk and reward have increased since the 2000s while references to power have decreased at a similar rate. (Note that error bars represent how much variance there is within groups, which allows you to eyeball the statistical significance of mean differences.)</p> <p>The shift in emphasis from power to risk-reward could reflect that commencement speakers are now focusing more abstractly on the potential benefits and hazards of life after graduation, whereas earlier speakers more narrowly focused on ambition and dominance (perhaps referring to power held by past alumni and projecting the potential for graduates to climb social ladders in the future). You could examine a sample of speeches that show this pattern most dramatically (speeches high in risk-reward and low in power in recent years, and vice versa for pre-2009 speeches) to help determine how these themes have shifted and what specific motives or framing devices seem to have been (de)emphasized.</p> In\u00a0[11]: Copied! <pre>import nltk\nfrom math import ceil\n\nnltk.download(\"punkt_tab\", quiet=True)\n\n\ndef count_words(text: str):\n    return len([token for token in nltk.word_tokenize(text) if token.isalnum()])\n\n\ndef split_text(text: str, bins=3):\n    sentences = nltk.sent_tokenize(text)\n    bin_size = ceil(count_words(text) / bins) + 1\n    text_parts = [[]]\n    word_counts = [0] * bins\n    current_bin = 0\n    for sentence in sentences:\n        sentence_size = count_words(sentence)\n        if (current_bin + 1) &lt; bins and (\n            word_counts[current_bin] + sentence_size\n        ) &gt; bin_size:\n            text_parts.append([])\n            current_bin += 1\n        word_counts[current_bin] += sentence_size\n        text_parts[current_bin].append(sentence)\n    return pandas.DataFrame(\n        {\n            \"text\": [\" \".join(x) for x in text_parts],\n            \"segment\": pandas.Series(range(bins)) + 1,\n            \"WC\": word_counts,\n        }\n    )\n\n\nsegmented_text = []\nfor i, text in enumerate(speeches[\"text\"]):\n    text_parts = split_text(text)\n    text_info = speeches.iloc[[i] * 3][[\"name\", \"school\", \"year\"]]\n    text_info.reset_index(drop=True, inplace=True)\n    segmented_text.append(pandas.concat([text_info, text_parts], axis=1))\nsegmented_text = pandas.concat(segmented_text)\nsegmented_text.reset_index(drop=True, inplace=True)\n\nsegmented_text.iloc[0:9, :6]\n</pre> import nltk from math import ceil  nltk.download(\"punkt_tab\", quiet=True)   def count_words(text: str):     return len([token for token in nltk.word_tokenize(text) if token.isalnum()])   def split_text(text: str, bins=3):     sentences = nltk.sent_tokenize(text)     bin_size = ceil(count_words(text) / bins) + 1     text_parts = [[]]     word_counts = [0] * bins     current_bin = 0     for sentence in sentences:         sentence_size = count_words(sentence)         if (current_bin + 1) &lt; bins and (             word_counts[current_bin] + sentence_size         ) &gt; bin_size:             text_parts.append([])             current_bin += 1         word_counts[current_bin] += sentence_size         text_parts[current_bin].append(sentence)     return pandas.DataFrame(         {             \"text\": [\" \".join(x) for x in text_parts],             \"segment\": pandas.Series(range(bins)) + 1,             \"WC\": word_counts,         }     )   segmented_text = [] for i, text in enumerate(speeches[\"text\"]):     text_parts = split_text(text)     text_info = speeches.iloc[[i] * 3][[\"name\", \"school\", \"year\"]]     text_info.reset_index(drop=True, inplace=True)     segmented_text.append(pandas.concat([text_info, text_parts], axis=1)) segmented_text = pandas.concat(segmented_text) segmented_text.reset_index(drop=True, inplace=True)  segmented_text.iloc[0:9, :6] Out[11]: name school year text segment WC 0 Aaron Sorkin Syracuse University 2012 Thank you very much. Madam Chancellor, members... 1 856 1 Aaron Sorkin Syracuse University 2012 The actor had been offered the lead role in a ... 2 827 2 Aaron Sorkin Syracuse University 2012 In that 11 years, I\u2019ve written three televisio... 3 880 3 Abigail Washburn Colorado College 2012 Bright morning stars are rising\\nBright mornin... 1 995 4 Abigail Washburn Colorado College 2012 I\u2019m standing here thinking\u2026 yea right, how am ... 2 934 5 Abigail Washburn Colorado College 2012 I looked around at the South Carolina party an... 3 1083 6 Adam Savage Sarah Lawrence College 2012 To President Lawrence, Chairman Hill, the Boar... 1 471 7 Adam Savage Sarah Lawrence College 2012 I decried and derided all of the skills I'd se... 2 461 8 Adam Savage Sarah Lawrence College 2012 I'll wager that at some point you'll have the ... 3 519 In\u00a0[12]: Copied! <pre>processed_segments = receptiviti.request(\n    segmented_text[\"text\"], version=\"v2\", context=\"spoken\"\n)\n\nprocessed_segments = receptiviti.request(\n    segmented_text[\"text\"], version=\"v2\", context=\"spoken\"\n)\nsegmented_text = pandas.concat([segmented_text, processed_segments], axis=1)\n\nsegmented_text.iloc[0:9, 8:]\n</pre> processed_segments = receptiviti.request(     segmented_text[\"text\"], version=\"v2\", context=\"spoken\" )  processed_segments = receptiviti.request(     segmented_text[\"text\"], version=\"v2\", context=\"spoken\" ) segmented_text = pandas.concat([segmented_text, processed_segments], axis=1)  segmented_text.iloc[0:9, 8:] Out[12]: summary.word_count summary.words_per_sentence summary.sentence_count summary.six_plus_words summary.capitals summary.emojis summary.emoticons summary.hashtags summary.urls big_5.extraversion ... disc_dimensions.people_relationship_emotion_focus disc_dimensions.task_system_object_focus disc_dimensions.d_axis disc_dimensions.i_axis disc_dimensions.s_axis disc_dimensions.c_axis disc_dimensions.d_axis_proportional disc_dimensions.i_axis_proportional disc_dimensions.s_axis_proportional disc_dimensions.c_axis_proportional 0 873 20.785714 42 0.182131 0.025233 0 0 0.000000 0 66.167710 ... 77.517546 38.989039 49.997553 70.498112 52.742495 37.405196 0.237356 0.334680 0.250388 0.177576 1 852 18.127660 47 0.213615 0.034065 0 0 0.000000 0 40.093982 ... 46.868716 47.558481 45.819695 45.486208 51.165188 51.540311 0.236170 0.234451 0.263723 0.265656 2 891 19.800000 45 0.181818 0.030344 0 0 0.000000 0 72.795664 ... 72.966879 54.987488 55.788997 64.265738 56.272576 48.850144 0.247756 0.285400 0.249903 0.216941 3 1030 35.517241 29 0.273786 0.024229 0 0 0.000000 0 58.482160 ... 66.958582 47.893402 52.189081 61.708492 53.739374 45.449313 0.244920 0.289594 0.252195 0.213291 4 953 25.078947 38 0.230850 0.040709 0 0 0.003148 0 54.971582 ... 50.129968 38.818173 40.352055 45.856070 53.946434 47.471349 0.215067 0.244402 0.287521 0.253011 5 1133 26.976190 42 0.233892 0.031886 0 0 0.000000 0 44.229974 ... 55.383952 38.426485 46.396992 55.701445 49.353260 41.109217 0.240947 0.289267 0.256299 0.213487 6 487 15.709677 31 0.250513 0.043437 0 0 0.000000 0 61.890701 ... 54.513837 53.597281 54.051544 54.511747 49.798123 49.377714 0.260190 0.262405 0.239715 0.237691 7 482 12.358974 39 0.215768 0.029583 0 0 0.000000 0 45.491979 ... 55.922241 42.280350 51.518933 59.250205 45.624964 39.671584 0.262764 0.302196 0.232702 0.202338 8 540 9.818182 55 0.200000 0.029501 0 0 0.000000 0 60.548414 ... 65.154960 56.856802 60.629919 64.903709 47.989630 44.829601 0.277669 0.297242 0.219780 0.205308 <p>9 rows \u00d7 217 columns</p> In\u00a0[13]: Copied! <pre># select the narrower SALLEE categories\nemotions = segmented_text.filter(regex=\"^sallee\").iloc[:, 6:]\n\n# correlate emotion scores with segment contrasts\n# and select the 5 most deviating emotions\nmost_deviating = emotions[\n    pandas.get_dummies(segmented_text[\"segment\"])\n    .apply(emotions.corrwith)\n    .abs()\n    .agg(\"max\", 1)\n    .sort_values(ascending=False)[:5]\n    .index\n]\n</pre> # select the narrower SALLEE categories emotions = segmented_text.filter(regex=\"^sallee\").iloc[:, 6:]  # correlate emotion scores with segment contrasts # and select the 5 most deviating emotions most_deviating = emotions[     pandas.get_dummies(segmented_text[\"segment\"])     .apply(emotions.corrwith)     .abs()     .agg(\"max\", 1)     .sort_values(ascending=False)[:5]     .index ] <p>Now we can look at those categories across segments:</p> In\u00a0[14]: Copied! <pre>from matplotlib.colors import ListedColormap\n\nsegment_data = most_deviating.groupby(segmented_text[\"segment\"])\n\nbars = segment_data.agg(\"mean\").T.plot.bar(\n    colormap=ListedColormap(colors[:3]),\n    yerr=(segment_data.agg(\"std\") / segment_data.agg(\"count\") ** 0.5).values,\n    capsize=3,\n    ylabel=\"Score\",\n    xlabel=\"Category\"\n)\n</pre> from matplotlib.colors import ListedColormap  segment_data = most_deviating.groupby(segmented_text[\"segment\"])  bars = segment_data.agg(\"mean\").T.plot.bar(     colormap=ListedColormap(colors[:3]),     yerr=(segment_data.agg(\"std\") / segment_data.agg(\"count\") ** 0.5).values,     capsize=3,     ylabel=\"Score\",     xlabel=\"Category\" ) <p>The bar chart displays original values, which offers the clearest view of how meaningful the differences between segments might be, in addition to their statistical significance (which offers a rough guide to the reliability of differences, based on the variance within and between segments). By looking at the bar graph, you can immediately see that admiration shows some of the starkest differences between middle and early/late segments.</p> In\u00a0[15]: Copied! <pre>scaled_summaries = (\n    (most_deviating - most_deviating.agg(\"mean\")) / most_deviating.agg(\"std\")\n).groupby(segmented_text[\"segment\"]).agg(\n    [\"mean\", \"size\"]\n)\n\nfig, ax = subplots()\nax.set(ylabel=\"Score (Scaled)\", xlabel=\"Segment\")\nfor i, cat in enumerate(most_deviating):\n    summary = scaled_summaries[cat]\n    ax.errorbar(\n        summary.index.astype(str),\n        summary[\"mean\"],\n        yerr=1 / summary[\"size\"] ** 0.5,\n        color=colors[i],\n        linestyle=linestyles[i],\n        label=cat,\n        capsize=4,\n    )\nlegend = fig.legend(loc=\"right\", bbox_to_anchor=(1.2, .5))\n</pre> scaled_summaries = (     (most_deviating - most_deviating.agg(\"mean\")) / most_deviating.agg(\"std\") ).groupby(segmented_text[\"segment\"]).agg(     [\"mean\", \"size\"] )  fig, ax = subplots() ax.set(ylabel=\"Score (Scaled)\", xlabel=\"Segment\") for i, cat in enumerate(most_deviating):     summary = scaled_summaries[cat]     ax.errorbar(         summary.index.astype(str),         summary[\"mean\"],         yerr=1 / summary[\"size\"] ** 0.5,         color=colors[i],         linestyle=linestyles[i],         label=cat,         capsize=4,     ) legend = fig.legend(loc=\"right\", bbox_to_anchor=(1.2, .5)) <p>The line charts, on the other hand, shows standardized values, effectively zooming in on the differences between segments. This more clearly shows, for example, that admiration and joy seem to be used as bookends in commencement speeches, peaking early and late, whereas more negative and intense emotions such as anger, disgust, and surprise peak in the middle section.</p>"},{"location":"articles/commencement_example/#data","title":"Data\u00b6","text":"<p>We'll start by collecting and processing the speeches.</p>"},{"location":"articles/commencement_example/#collection","title":"Collection\u00b6","text":"<p>The speeches used to be provided more directly, but the service hosting them has since shut down.</p> <p>They are still available in a slightly less convenient form, as the source of a site that displays them: whatrocks.github.io/commencement-db.</p> <p>First, we can retrieve metadata from a separate repository:</p>"},{"location":"articles/commencement_example/#text-preparation","title":"Text Preparation\u00b6","text":"<p>Now we can read in the texts and associate them with their metadata:</p>"},{"location":"articles/commencement_example/#load-package","title":"Load Package\u00b6","text":"<p>If this is your first time using the package, see the Get Started guide to install it and set up your API credentials.</p>"},{"location":"articles/commencement_example/#analyze-full-texts","title":"Analyze Full Texts\u00b6","text":"<p>We might start by seeing if any speeches stand out in terms of language style, or if there are any trends in content over time.</p>"},{"location":"articles/commencement_example/#full-process-text","title":"Full: Process Text\u00b6","text":"<p>Now we can send the texts to the API for scoring, and join the results we get to the metadata:</p>"},{"location":"articles/commencement_example/#full-analyze-style","title":"Full: Analyze Style\u00b6","text":"<p>To get at stylistic uniqueness, we can calculate Language Style Matching between each speech and the mean of all speeches:</p>"},{"location":"articles/commencement_example/#full-analyze-content","title":"Full: Analyze Content\u00b6","text":"<p>To look at content over time, we might focus on a potentially interesting framework, such as drives:</p>"},{"location":"articles/commencement_example/#analyze-segments","title":"Analyze Segments\u00b6","text":"<p>Another thing we might look for is trends within each speech. For instance, are there common emotional trajectories over the course of a speech?</p> <p>One way to look at this would be to split texts into roughly equal sizes, and score each section:</p>"},{"location":"articles/commencement_example/#segments-process-text","title":"Segments: Process Text\u00b6","text":"<p>Now we can send each segment to the API to be scored:</p>"},{"location":"articles/commencement_example/#segments-analyze-scores","title":"Segments: Analyze Scores\u00b6","text":"<p>The SALLEE framework offers measures of emotions, so we might see which categories deviate the most in any of their segments:</p>"},{"location":"articles/quick_start/","title":"Get Started","text":"In\u00a0[2]: Copied! <pre>import receptiviti\n</pre> import receptiviti In\u00a0[3]: Copied! <pre>receptiviti.status()\n</pre> receptiviti.status() <pre>Status: OK\nMessage: 200: Hello there, World!\n</pre> Out[3]: <pre>&lt;Response [200]&gt;</pre> <p>If your credentials are not recognized, you'll get a response like this:</p> In\u00a0[4]: Copied! <pre>receptiviti.status(key=123, secret=123)\n</pre> receptiviti.status(key=123, secret=123) <pre>Status: ERROR\nMessage: 401 (1411): Unrecognized API key pair. This call will not count towards your plan.\n</pre> Out[4]: <pre>&lt;Response [401]&gt;</pre> In\u00a0[5]: Copied! <pre>results = receptiviti.request(\"texts to score\")\n</pre> results = receptiviti.request(\"texts to score\") <p>Or a character vector:</p> In\u00a0[6]: Copied! <pre>results = receptiviti.request([\"text one\", \"text two\"])\n</pre> results = receptiviti.request([\"text one\", \"text two\"]) <p>Or from a <code>DataFrame</code>:</p> In\u00a0[7]: Copied! <pre>import pandas\ndata = pandas.DataFrame({\"text\": [\"text a\", \"text b\"]})\n\n# directly\nresults = receptiviti.request(data[\"text\"])\n\n# by column name\nresults = receptiviti.request(data, text_column=\"text\")\n</pre> import pandas data = pandas.DataFrame({\"text\": [\"text a\", \"text b\"]})  # directly results = receptiviti.request(data[\"text\"])  # by column name results = receptiviti.request(data, text_column=\"text\") In\u00a0[8]: Copied! <pre># single\nresults = receptiviti.request(\"files/file.txt\")\n\n# multiple\nresults = receptiviti.request(\n  files = [\"files/file1.txt\", \"files/file2.txt\"]\n)\n</pre> # single results = receptiviti.request(\"files/file.txt\")  # multiple results = receptiviti.request(   files = [\"files/file1.txt\", \"files/file2.txt\"] ) <p>Or to a comma delimited file with a column containing text. Here, the <code>text_column</code> argument specifies which column contains text:</p> In\u00a0[9]: Copied! <pre># single\nresults = receptiviti.request(\"files/file.csv\", text_column=\"text\")\n\n# multiple\nresults = receptiviti.request(\n  files = [\"files/file1.csv\", \"files/file2.csv\"],\n  text_column=\"text\"\n)\n</pre> # single results = receptiviti.request(\"files/file.csv\", text_column=\"text\")  # multiple results = receptiviti.request(   files = [\"files/file1.csv\", \"files/file2.csv\"],   text_column=\"text\" ) <p>Or you can point to a directory containing text files:</p> In\u00a0[10]: Copied! <pre>results = receptiviti.request(directory = \"files\")\n</pre> results = receptiviti.request(directory = \"files\") <p>By default <code>.txt</code> files will be looked for, but you can specify <code>.csv</code> files with the <code>file_type</code> argument:</p> In\u00a0[11]: Copied! <pre>results = receptiviti.request(\n  directory = \"files\",\n  text_column=\"text\", file_type=\"csv\"\n)\n</pre> results = receptiviti.request(   directory = \"files\",   text_column=\"text\", file_type=\"csv\" ) In\u00a0[12]: Copied! <pre>results = receptiviti.request(\"texts to score\")\nresults.iloc[:, :3]\n</pre> results = receptiviti.request(\"texts to score\") results.iloc[:, :3] Out[12]: text_hash summary.word_count summary.words_per_sentence 0 acab8277267d0efee0828f94e0919ddf 3 3 <p>Here, the first column (<code>text_hash</code>) is the MD5 hash of the text, which identifies unique texts, and is stored in the main cache.</p> <p>The entered text can also be included with the <code>return_text</code> argument:</p> In\u00a0[13]: Copied! <pre>results = receptiviti.request(\"texts to score\", return_text=True)\nresults[[\"text_hash\", \"text\"]]\n</pre> results = receptiviti.request(\"texts to score\", return_text=True) results[[\"text_hash\", \"text\"]] Out[13]: text_hash text 0 acab8277267d0efee0828f94e0919ddf texts to score <p>You can also select frameworks before they are all returned:</p> In\u00a0[14]: Copied! <pre>results = receptiviti.request(\"texts to score\", frameworks=\"liwc\")\nresults.iloc[:, :5]\n</pre> results = receptiviti.request(\"texts to score\", frameworks=\"liwc\") results.iloc[:, :5] Out[14]: text_hash analytical_thinking clout authentic emotional_tone 0 acab8277267d0efee0828f94e0919ddf 0.99 0.5 0.01 0.257742 <p>By default, a single framework will have column names without the framework name, but you can retain these with <code>framework_prefix=True</code>:</p> In\u00a0[15]: Copied! <pre>results = receptiviti.request(\n  \"texts to score\",\n  frameworks=\"liwc\", framework_prefix=True\n)\nresults.iloc[:, :4]\n</pre> results = receptiviti.request(   \"texts to score\",   frameworks=\"liwc\", framework_prefix=True ) results.iloc[:, :4] Out[15]: text_hash liwc.analytical_thinking liwc.clout liwc.authentic 0 acab8277267d0efee0828f94e0919ddf 0.99 0.5 0.01 In\u00a0[16]: Copied! <pre>data = pandas.DataFrame({\n  \"id\": [1, 2, 3, 4],\n  \"text\": [\"text a\", float(\"nan\"), \"\", \"text a\"]\n})\nresults = receptiviti.request(data[\"text\"])\n\n# combine data and results\ndata.join(results).iloc[:, :5]\n</pre> data = pandas.DataFrame({   \"id\": [1, 2, 3, 4],   \"text\": [\"text a\", float(\"nan\"), \"\", \"text a\"] }) results = receptiviti.request(data[\"text\"])  # combine data and results data.join(results).iloc[:, :5] Out[16]: id text text_hash summary.word_count summary.words_per_sentence 0 1 text a 42ff59040f004970040f90a19aa6b3fa 2.0 2.0 1 2 NaN NaN NaN NaN 2 3 NaN NaN NaN 3 4 text a 42ff59040f004970040f90a19aa6b3fa 2.0 2.0 <p>You can also provide a vector of unique IDs to be returned with results so they can be merged with other data:</p> In\u00a0[17]: Copied! <pre>results = receptiviti.request([\"text a\", \"text b\"], ids=[\"a\", \"b\"])\nresults.iloc[:, :4]\n</pre> results = receptiviti.request([\"text a\", \"text b\"], ids=[\"a\", \"b\"]) results.iloc[:, :4] Out[17]: id text_hash summary.word_count summary.words_per_sentence 0 a 42ff59040f004970040f90a19aa6b3fa 2 2 1 b 4db2bfd2c8140dffac0060c9fb1c6d6f 2 2 In\u00a0[18]: Copied! <pre># merge with a new dataset\ndata = pandas.DataFrame({\n  \"id\": [\"a1\", \"b1\", \"a2\", \"b2\"],\n  \"type\": [\"a\", \"b\", \"a\", \"b\"]\n})\ndata.join(results.set_index(\"id\"), \"type\").iloc[:, :5]\n</pre> # merge with a new dataset data = pandas.DataFrame({   \"id\": [\"a1\", \"b1\", \"a2\", \"b2\"],   \"type\": [\"a\", \"b\", \"a\", \"b\"] }) data.join(results.set_index(\"id\"), \"type\").iloc[:, :5] Out[18]: id type text_hash summary.word_count summary.words_per_sentence 0 a1 a 42ff59040f004970040f90a19aa6b3fa 2 2 1 b1 b 4db2bfd2c8140dffac0060c9fb1c6d6f 2 2 2 a2 a 42ff59040f004970040f90a19aa6b3fa 2 2 3 b2 b 4db2bfd2c8140dffac0060c9fb1c6d6f 2 2 In\u00a0[19]: Copied! <pre>receptiviti.request(\"texts to score\", \"~/Documents/results.csv\", overwrite=True)\nresults = pandas.read_csv(\"~/Documents/results.csv\")\nresults.iloc[:, :4]\n</pre> receptiviti.request(\"texts to score\", \"~/Documents/results.csv\", overwrite=True) results = pandas.read_csv(\"~/Documents/results.csv\") results.iloc[:, :4] Out[19]: id text_hash summary.word_count summary.words_per_sentence 0 1 acab8277267d0efee0828f94e0919ddf 3 3"},{"location":"articles/quick_start/#install-and-load","title":"Install and Load\u00b6","text":"<p>First, download and install Python from python.org.</p> <p>Then, install the package:</p> <pre>pip install git+https://github.com/receptiviti/receptiviti-python.git\n</pre> <p>Each time you start a Python session, load the package:</p>"},{"location":"articles/quick_start/#set-up-api-credentials","title":"Set Up API Credentials\u00b6","text":"<p>You can find your API key and secret on your dashboard.</p> <p>You can set these credentials up in Python permanently or temporarily:</p>"},{"location":"articles/quick_start/#permanent","title":"Permanent\u00b6","text":"<p>Open or create a <code>~/.env</code> file, Then add these environment variables with your key and secret:</p> <pre>RECEPTIVITI_KEY=\"\"\nRECEPTIVITI_SECRET=\"\"\n</pre> <p>These can be read in with the <code>receptiviti.readin_env()</code> function, which is automatically called if credentials are not otherwise provided (and the <code>dotenv</code> argument is <code>True</code>).</p>"},{"location":"articles/quick_start/#temporary","title":"Temporary\u00b6","text":"<p>Add your key and secret, and run at the start of each session:</p> <pre>import os\nos.environ[\"RECEPTIVITI_KEY\"]=\"32lettersandnumbers\"\nos.environ[\"RECEPTIVITI_SECRET\"]=\"56LettersAndNumbers\"\n</pre>"},{"location":"articles/quick_start/#confirm-credentials","title":"Confirm Credentials\u00b6","text":"<p>Check that the API is reachable, and your credentials are recognized:</p>"},{"location":"articles/quick_start/#enter-your-text","title":"Enter Your Text\u00b6","text":""},{"location":"articles/quick_start/#loaded-text","title":"Loaded Text\u00b6","text":"<p>If your texts are already in Python, you can enter them directly.</p> <p>These can be in a single character:</p>"},{"location":"articles/quick_start/#text-in-files","title":"Text in files\u00b6","text":"<p>You can enter paths to files containing separate texts in each line:</p>"},{"location":"articles/quick_start/#use-results","title":"Use Results\u00b6","text":""},{"location":"articles/quick_start/#returned-results","title":"Returned Results\u00b6","text":"<p>Results are returned as a <code>DataFrame</code>, with a row for each text, and columns for each framework variable:</p>"},{"location":"articles/quick_start/#aligning-results","title":"Aligning Results\u00b6","text":"<p>Results are returned in a way that aligns with the text you enter originally, including any duplicates or invalid entries.</p> <p>This means you can add the results object to original data:</p>"},{"location":"articles/quick_start/#saved-results","title":"Saved Results\u00b6","text":"<p>Results can also be saved to a <code>.csv</code> file:</p>"},{"location":"articles/quick_start/#preserving-results","title":"Preserving Results\u00b6","text":"<p>The <code>receptiviti.request</code> function tries to avoid sending texts to the API as much as possible:</p> <ul> <li>As part of the preparation process, it excludes duplicates and invalid texts.</li> <li>If enabled, it checks the primary cache to see if any texts have already been scored.<ul> <li>The primary cache is an Arrow database located by the <code>cache</code> augment.</li> <li>Its format is determined by <code>cache_format</code>.</li> <li>You can skip checking it initially while still writing results to it with <code>cache_overwrite=True</code>.</li> <li>It can be cleared with <code>clear_cache=True</code>.</li> </ul> </li> <li>It will check for any responses to previous, identical requests.<ul> <li>Responses are stored in the <code>receptiviti_request_cache</code> directory of your system's temporary directory (<code>tempfile.gettempdir()</code>).</li> <li>You can avoid using this cache with <code>request_cache=False</code>.</li> <li>This cache is cleared after a day.</li> </ul> </li> </ul> <p>If you want to make sure no texts are sent to the API, you can use <code>make_request=False</code>. This will use the primary and request cache, but will fail if any texts are not found there.</p> <p>If a call fails before results can be written to the cache or returned, all received responses will still be in the request cache, but those will be deleted after a day.</p>"},{"location":"articles/quick_start/#handling-big-data","title":"Handling Big Data\u00b6","text":"<p>The <code>receptiviti.request</code> function will handle splitting texts into bundles, so the limit on how many texts you can process at once will come down to your system's amount of random access memory (RAM). Several thousands of texts should be fine, but getting into millions of texts, you may not be able to have all of the results loaded at once. To get around this, you can fully process subsets of your texts.</p> <p>A benefit of processing more texts at once is that requests can be parallelized, but this is more RAM intensive, and the primary cache is updated less frequently (as it is updated only at the end of a complete run).</p> <p>You could also parallelize your own batches, but be sure to set <code>cores</code> to <code>1</code> (to disable the function's parallelization) and do not enable the primary cache (to avoid attempting to read from the cache while it is being written to by another instance).</p> <p>Not using the primary cache is also more efficient, but you may want to ensure you are not sending duplicate texts between calls. The function handles duplicate texts within calls (only ever sending unique texts), but depends on the cache to avoid sending duplicates between calls.</p>"},{"location":"articles/special_cases/","title":"Special Cases","text":"In\u00a0[2]: Copied! <pre>import receptiviti\nimport pandas\n\ntext = \"Text to be normed differently.\"\nwritten = receptiviti.request(text, version=\"v2\")\nspoken = receptiviti.request(text, version=\"v2\", context=\"spoken\")\n\n# select a few categories that differ between contexts\ndiffering = written.columns[(written != spoken).iloc[0]][0:10]\n\n# note that the text hashes are sensitive to the set context\npandas.concat([written, spoken], ignore_index=True)[differing].rename(\n    {0: \"written\", 1: \"spoken\"}\n).T\n</pre> import receptiviti import pandas  text = \"Text to be normed differently.\" written = receptiviti.request(text, version=\"v2\") spoken = receptiviti.request(text, version=\"v2\", context=\"spoken\")  # select a few categories that differ between contexts differing = written.columns[(written != spoken).iloc[0]][0:10]  # note that the text hashes are sensitive to the set context pandas.concat([written, spoken], ignore_index=True)[differing].rename(     {0: \"written\", 1: \"spoken\"} ).T Out[2]: written spoken text_hash 45bbf3011f5ef8bb027af931a202afc6 d89df4983ee8808bc1006c014ae142ca context written spoken big_5.extraversion 29.60482 31.146502 big_5.active 41.924956 42.616414 big_5.assertive 18.603455 20.813242 big_5.cheerful 41.649146 38.750138 big_5.energetic 60.83928 65.029776 big_5.friendly 44.94581 43.746682 big_5.sociable 19.6961 15.648814 big_5.openness 11.894965 9.808802 In\u00a0[3]: Copied! <pre>context_text = [\"Text with normed in it.\", \"Text with differently in it.\"]\n\n# set lower word count filter for this toy sample\ncontext_status = receptiviti.norming(\n    name=\"custom_example\",\n    text=context_text,\n    options={\"min_word_count\": 1},\n    verbose=False,\n)\n\n# the `second_pass` result shows what was analyzed\ncontext_status[\"second_pass\"]\n</pre> context_text = [\"Text with normed in it.\", \"Text with differently in it.\"]  # set lower word count filter for this toy sample context_status = receptiviti.norming(     name=\"custom_example\",     text=context_text,     options={\"min_word_count\": 1},     verbose=False, )  # the `second_pass` result shows what was analyzed context_status[\"second_pass\"] Out[3]: submitted_samples analyzed_samples analyzed_word_count filtered_blank filtered_word_count filtered_punctuation 0 2 2 10 0 0 0 <p>Then use the <code>custom_context</code> argument to specify that norming context when scoring:</p> In\u00a0[4]: Copied! <pre>custom = receptiviti.request(text, version=\"v2\", custom_context=\"custom_example\")\n\ncustom[differing].rename({0: \"custom\"}).T\n</pre> custom = receptiviti.request(text, version=\"v2\", custom_context=\"custom_example\")  custom[differing].rename({0: \"custom\"}).T Out[4]: custom text_hash 3dee016e73a68569d7cd31e1564c84a3 context custom/custom_example big_5.extraversion 0 big_5.active 0 big_5.assertive 100 big_5.cheerful 100 big_5.energetic 100 big_5.friendly 100 big_5.sociable 0 big_5.openness 0 In\u00a0[5]: Copied! <pre>from os import makedirs\n\nbase_dir = \"../../../\"\ntext_dir = base_dir + \"test_texts\"\nmakedirs(text_dir, exist_ok=True)\n\nfor i in range(10):\n    with open(f\"{text_dir}/example_{i}.txt\", \"w\", encoding=\"utf-8\") as file:\n        file.write(f\"An example text {i}.\")\n</pre> from os import makedirs  base_dir = \"../../../\" text_dir = base_dir + \"test_texts\" makedirs(text_dir, exist_ok=True)  for i in range(10):     with open(f\"{text_dir}/example_{i}.txt\", \"w\", encoding=\"utf-8\") as file:         file.write(f\"An example text {i}.\") <p>And then minimally load these and their results by saving results to a Parquet dataset.</p> <p>Disabling the <code>request_cache</code> will also avoid storing a copy of raw results.</p> In\u00a0[6]: Copied! <pre>import receptiviti\n\ndb_dir = base_dir + \"test_results\"\nmakedirs(db_dir, exist_ok=True)\n\nreceptiviti.request(\n  directory=text_dir, collect_results=False, cache=db_dir, request_cache=False\n)\n</pre> import receptiviti  db_dir = base_dir + \"test_results\" makedirs(db_dir, exist_ok=True)  receptiviti.request(   directory=text_dir, collect_results=False, cache=db_dir, request_cache=False ) <p>Results are now available in the cache directory, which you can load in using the request function again:</p> In\u00a0[7]: Copied! <pre># adding make_request=False just ensures requests are not made if not found\nresults = receptiviti.request(directory=text_dir, cache=db_dir, make_request=False)\nresults.iloc[:, 0:3]\n</pre> # adding make_request=False just ensures requests are not made if not found results = receptiviti.request(directory=text_dir, cache=db_dir, make_request=False) results.iloc[:, 0:3] Out[7]: id text_hash summary.word_count 0 ../../../test_texts\\example_0.txt 1079592e75e8857b012e4d9bae525de4 4 1 ../../../test_texts\\example_1.txt 5f9b76d1c846acd9d5b7c8c91e230114 4 2 ../../../test_texts\\example_2.txt 87682e4113d7bf58377e1f2c6037bc99 4 3 ../../../test_texts\\example_3.txt 99f34e51e2e12696b153e664fd01ed9c 4 4 ../../../test_texts\\example_4.txt db8f2e1d82674f51920c2e6b9d1e0f57 4 5 ../../../test_texts\\example_5.txt b7ed3ead170a92fbdc7b945a067dd181 4 6 ../../../test_texts\\example_6.txt 1ae462819e36f657305695a8ac63858a 4 7 ../../../test_texts\\example_7.txt 7c3bb4bd1dd6955041e2649cc7e99c51 4 8 ../../../test_texts\\example_8.txt 6382fa2141d3ae2b313c9295fd76aaf1 4 9 ../../../test_texts\\example_9.txt bb9d24d530c37a7e2e5b8e7f852abe43 4 In\u00a0[8]: Copied! <pre>res_dir = base_dir + \"text_results_manual\"\nmakedirs(res_dir, exist_ok=True)\n\n# using the same files as before\nfiles = [f\"{text_dir}/{file}\" for file in os.listdir(text_dir)]\n\n# process 5 files at a time\nfor i in range(0, len(files), 5):\n  file_subset = files[i : i + 5]\n  results = receptiviti.request(\n    files=file_subset, ids=file_subset, cores=1, cache=False, request_cache=False\n  )\n  results.to_csv(f\"{res_dir}/files_{i}-{i + 5}.csv.xz\", index=False)\n</pre> res_dir = base_dir + \"text_results_manual\" makedirs(res_dir, exist_ok=True)  # using the same files as before files = [f\"{text_dir}/{file}\" for file in os.listdir(text_dir)]  # process 5 files at a time for i in range(0, len(files), 5):   file_subset = files[i : i + 5]   results = receptiviti.request(     files=file_subset, ids=file_subset, cores=1, cache=False, request_cache=False   )   results.to_csv(f\"{res_dir}/files_{i}-{i + 5}.csv.xz\", index=False) <p>Now results will be stored in smaller files:</p> In\u00a0[9]: Copied! <pre>from pandas import read_csv\n\nread_csv(f\"{res_dir}/files_0-5.csv.xz\").iloc[:, 0:3]\n</pre> from pandas import read_csv  read_csv(f\"{res_dir}/files_0-5.csv.xz\").iloc[:, 0:3] Out[9]: id text_hash summary.word_count 0 ../../../test_texts/example_0.txt 1079592e75e8857b012e4d9bae525de4 4 1 ../../../test_texts/example_1.txt 5f9b76d1c846acd9d5b7c8c91e230114 4 2 ../../../test_texts/example_2.txt 87682e4113d7bf58377e1f2c6037bc99 4 3 ../../../test_texts/example_3.txt 99f34e51e2e12696b153e664fd01ed9c 4 4 ../../../test_texts/example_4.txt db8f2e1d82674f51920c2e6b9d1e0f57 4"},{"location":"articles/special_cases/#special-cases","title":"Special Cases\u00b6","text":""},{"location":"articles/special_cases/#norming-contexts","title":"Norming Contexts\u00b6","text":"<p>Some measures are normed against a sample of text. These samples may be more or less appropriate to your texts.</p>"},{"location":"articles/special_cases/#built-in","title":"Built-In\u00b6","text":"<p>The default context is meant for general written text, and there is another built-in context for general spoken text:</p>"},{"location":"articles/special_cases/#custom","title":"Custom\u00b6","text":"<p>You can also norm against your own sample, which involves first establishing a context, then scoring against it.</p> <p>Use the <code>receptiviti.norming</code> function to establish a custom context:</p>"},{"location":"articles/special_cases/#high-volume","title":"High Volume\u00b6","text":"<p>The Receptiviti API has limits on bundle requests, so the <code>receptiviti.request()</code> function splits texts into acceptable bundles, to be spread across multiple requests.</p> <p>This means the only remaining limitation on the number of texts that can be processed comes from the memory of the system sending requests.</p> <p>The basic way to work around this limitation is to fully process smaller chunks of text.</p> <p>There are a few ways to avoid loading all texts and results.</p>"},{"location":"articles/special_cases/#cache-as-output","title":"Cache as Output\u00b6","text":"<p>Setting the <code>collect_results</code> argument to <code>False</code> avoids retaining all batch results in memory as they are receive, but means results are not returned, so the they have to be collected in the cache.</p> <p>If texts are also too big to load into memory, they can be loaded from files at request time. By default, when multiple files pointed to as <code>text</code>, the actual texts are only loaded when they are being sent for scoring, which means only <code>bundle_size</code> * <code>cores</code> texts are loaded at a time.</p> <p>We can start by writing some small text examples to files:</p>"},{"location":"articles/special_cases/#manual-chunking","title":"Manual Chunking\u00b6","text":"<p>A more flexible approach would be to process smaller chunks of text normally, and handle loading and storing results yourself.</p> <p>In this case, it may be best to disable parallelization, and explicitly disable the primary cache (in case it's specified in an environment variable).</p>"},{"location":"functions/frameworks/","title":"Frameworks","text":"<p>Check the status of the API.</p>"},{"location":"functions/frameworks/#receptiviti.frameworks.frameworks","title":"<code>frameworks(url=os.getenv('RECEPTIVITI_URL', ''), key=os.getenv('RECEPTIVITI_KEY', ''), secret=os.getenv('RECEPTIVITI_SECRET', ''), dotenv=False)</code>","text":"<p>List Available Frameworks.</p> <p>Retrieve a list of all frameworks available to your account.</p> PARAMETER DESCRIPTION <p>The URL of the API.</p> <p> TYPE: <code>str</code> DEFAULT: <code>getenv('RECEPTIVITI_URL', '')</code> </p> <p>Your API key.</p> <p> TYPE: <code>str</code> DEFAULT: <code>getenv('RECEPTIVITI_KEY', '')</code> </p> <p>Your API secret.</p> <p> TYPE: <code>str</code> DEFAULT: <code>getenv('RECEPTIVITI_SECRET', '')</code> </p> <p>Path to a .env file to read environment variables from, or <code>True</code> to look for a file in the current directory.</p> <p> TYPE: <code>bool | str</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>List[str]</code> <p>List of framework names.</p> <p>Examples:</p> <pre><code>receptiviti.frameworks()\n</code></pre> Source code in <code>src\\receptiviti\\frameworks.py</code> <pre><code>def frameworks(\n    url: str = os.getenv(\"RECEPTIVITI_URL\", \"\"),\n    key: str = os.getenv(\"RECEPTIVITI_KEY\", \"\"),\n    secret: str = os.getenv(\"RECEPTIVITI_SECRET\", \"\"),\n    dotenv: Union[bool, str] = False,\n) -&gt; List[str]:\n    \"\"\"\n    List Available Frameworks.\n\n    Retrieve a list of all frameworks available to your account.\n\n    Args:\n      url (str): The URL of the API.\n      key (str): Your API key.\n      secret (str): Your API secret.\n      dotenv (bool | str): Path to a .env file to read environment variables from, or `True`\n        to look for a file in the current directory.\n\n    Returns:\n      List of framework names.\n\n    Examples:\n        ```python\n        receptiviti.frameworks()\n        ```\n    \"\"\"\n    _, url, key, secret = _resolve_request_def(url, key, secret, dotenv)\n    res = requests.get(url.lower() + \"/v2/frameworks\", auth=(key, secret), timeout=9999)\n    content = res.json() if res.text[:1] == \"[\" else {\"message\": res.text}\n    if res.status_code != 200:\n        msg = f\"Request Error ({res.status_code!s})\" + (\n            (\" (\" + str(content[\"code\"]) + \")\" if \"code\" in content else \"\") + \": \" + content[\"message\"]\n        )\n        raise RuntimeError(msg)\n    return content if isinstance(content, list) else []\n</code></pre>"},{"location":"functions/frameworks/#receptiviti.frameworks.frameworks(url)","title":"<code>url</code>","text":""},{"location":"functions/frameworks/#receptiviti.frameworks.frameworks(key)","title":"<code>key</code>","text":""},{"location":"functions/frameworks/#receptiviti.frameworks.frameworks(secret)","title":"<code>secret</code>","text":""},{"location":"functions/frameworks/#receptiviti.frameworks.frameworks(dotenv)","title":"<code>dotenv</code>","text":""},{"location":"functions/norming/","title":"Norming","text":"<p>Interact with the norming endpoint.</p>"},{"location":"functions/norming/#receptiviti.norming.norming","title":"<code>norming(name=None, text=None, options=None, delete=False, name_only=False, dotenv=True, key=os.getenv('RECEPTIVITI_KEY', ''), secret=os.getenv('RECEPTIVITI_SECRET', ''), url=os.getenv('RECEPTIVITI_URL', ''), verbose=True, **kwargs)</code>","text":"<p>View or Establish Custom Norming Contexts.</p> <p>Custom norming contexts can be used to process later texts by specifying the <code>custom_context</code> API argument in the <code>receptiviti.request</code> function (e.g., <code>receptiviti.request(\"text to score\", version=\"v2\", custom_context=\"norm_name\")</code>, where <code>norm_name</code> is the name you set here).</p> PARAMETER DESCRIPTION <p>Name of a new norming context, to be established from the provided 'text'. Not providing a name will list the previously created contexts.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <p>Text to be processed and used as the custom norming context. Not providing text will return the status of the named norming context.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <p>Options to set for the norming context (e.g., <code>{\"min_word_count\": 350, \"max_punctuation\": .25}</code>).</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <p>If <code>True</code>, will request removal of the <code>name</code> context.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>If <code>True</code>, will return a list of context names only, including those of build-in contexts.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Path to a .env file to read environment variables from. By default, will for a file in the current directory or <code>~/Documents</code>. Passed to <code>readin_env</code> as <code>path</code>.</p> <p> TYPE: <code>bool | str</code> DEFAULT: <code>True</code> </p> <p>Your API key.</p> <p> TYPE: <code>str</code> DEFAULT: <code>getenv('RECEPTIVITI_KEY', '')</code> </p> <p>Your API secret.</p> <p> TYPE: <code>str</code> DEFAULT: <code>getenv('RECEPTIVITI_SECRET', '')</code> </p> <p>The URL of the API; defaults to <code>https://api.receptiviti.com</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>getenv('RECEPTIVITI_URL', '')</code> </p> <p>If <code>False</code>, will not show status messages.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <p>Additional arguments to specify how texts are read in and processed (excluding than <code>cores</code>); see receptiviti.request.</p> <p> TYPE: <code>Any</code> DEFAULT: <code>{}</code> </p> RETURNS DESCRIPTION <code>Union[None, List[str], DataFrame, Series, Dict[str, Union[str, Series, DataFrame, None]]]</code> <p>Nothing if <code>delete</code> is <code>True</code>. If <code>name_only</code> is <code>True</code>, a <code>list</code> containing context names (built-in and custom). Otherwise, either a <code>pandas.DataFrame</code> containing all existing custom context statuses (if no <code>name</code> is specified), a <code>pandas.Series</code> containing the the status of <code>name</code> (if <code>text</code> is not specified), a dictionary:</p> <ul> <li><code>initial_status</code>: Initial status of the context.</li> <li><code>first_pass</code>: Response after texts are sent the first time, or   <code>None</code> if the initial status is <code>pass_two</code>.</li> <li><code>second_pass</code>: Response after texts are sent the second time.</li> </ul> <p>Examples:</p> <pre><code># list all available contexts:\nreceptiviti.norming()\n\n# create or get the status of a single context:\nreceptiviti.norming(\"new_context\")\n</code></pre> <p>Send texts to establish the context, just like the receptiviti.request function. <pre><code>## such as directly:\nreceptiviti.norming(\"new_context\", [\"text to send\", \"another text\"])\n\n## or from a file:\nreceptiviti.norming(\"new_context\", \"./path/to/file.csv\", text_column = \"text\")\n\n## delete the new context:\nreceptiviti.norming(\"new_context\", delete=True)\n</code></pre></p> Source code in <code>src\\receptiviti\\norming.py</code> <pre><code>def norming(\n    name: Union[str, None] = None,\n    text: Union[str, List[str], pandas.DataFrame, None] = None,\n    options: Union[Dict[str, Union[str, int]], None] = None,\n    delete: bool = False,\n    name_only: bool = False,\n    dotenv: Union[bool, str] = True,\n    key: str = os.getenv(\"RECEPTIVITI_KEY\", \"\"),\n    secret: str = os.getenv(\"RECEPTIVITI_SECRET\", \"\"),\n    url: str = os.getenv(\"RECEPTIVITI_URL\", \"\"),\n    verbose: bool = True,\n    **kwargs,\n) -&gt; Union[\n    None, List[str], pandas.DataFrame, pandas.Series, Dict[str, Union[str, pandas.Series, pandas.DataFrame, None]]\n]:\n    \"\"\"\n    View or Establish Custom Norming Contexts.\n\n    Custom norming contexts can be used to process later texts by specifying the\n    `custom_context` API argument in the `receptiviti.request` function (e.g.,\n    `receptiviti.request(\"text to score\", version=\"v2\", custom_context=\"norm_name\")`,\n    where `norm_name` is the name you set here).\n\n    Args:\n        name (str): Name of a new norming context, to be established from the provided 'text'.\n            Not providing a name will list the previously created contexts.\n        text (str): Text to be processed and used as the custom norming context.\n            Not providing text will return the status of the named norming context.\n        options (dict): Options to set for the norming context (e.g.,\n            `{\"min_word_count\": 350, \"max_punctuation\": .25}`).\n        delete (bool): If `True`, will request removal of the `name` context.\n        name_only (bool): If `True`, will return a list of context names only, including those of\n            build-in contexts.\n        dotenv (bool | str): Path to a .env file to read environment variables from. By default,\n            will for a file in the current directory or `~/Documents`.\n            Passed to `readin_env` as `path`.\n        key (str): Your API key.\n        secret (str): Your API secret.\n        url (str): The URL of the API; defaults to `https://api.receptiviti.com`.\n        verbose (bool): If `False`, will not show status messages.\n        **kwargs (Any): Additional arguments to specify how texts are read in and processed\n            (excluding than `cores`);\n            see [receptiviti.request][receptiviti.request].\n\n    Returns:\n        Nothing if `delete` is `True`.\n            If `name_only` is `True`, a `list` containing context names (built-in and custom).\n            Otherwise, either a `pandas.DataFrame` containing all existing custom context statuses\n            (if no `name` is specified), a `pandas.Series` containing the the status of\n            `name` (if `text` is not specified), a dictionary:\n\n            - `initial_status`: Initial status of the context.\n            - `first_pass`: Response after texts are sent the first time, or\n              `None` if the initial status is `pass_two`.\n            - `second_pass`: Response after texts are sent the second time.\n\n    Examples:\n        ```python\n        # list all available contexts:\n        receptiviti.norming()\n\n        # create or get the status of a single context:\n        receptiviti.norming(\"new_context\")\n        ```\n\n        Send texts to establish the context, just like\n        the [receptiviti.request][receptiviti.request] function.\n        ```python\n        ## such as directly:\n        receptiviti.norming(\"new_context\", [\"text to send\", \"another text\"])\n\n        ## or from a file:\n        receptiviti.norming(\"new_context\", \"./path/to/file.csv\", text_column = \"text\")\n\n        ## delete the new context:\n        receptiviti.norming(\"new_context\", delete=True)\n        ```\n    \"\"\"\n    kwargs[\"cores\"] = 1\n    _, url, key, secret = _resolve_request_def(url, key, secret, dotenv)\n    auth = requests.auth.HTTPBasicAuth(key, secret)\n    if name_only:\n        if verbose:\n            print(\"requesting list of existing custom norming contests\")\n        req = requests.get(url + \"/v2/norming/\", auth=auth, timeout=9999)\n        if req.status_code != 200:\n            msg = f\"failed to make norming list request: {req.status_code} {req.reason}\"\n            raise RuntimeError(msg)\n        norms = req.json()\n        if norms and verbose:\n            custom_prefix = re.compile(\"^custom/\")\n            print(\"available norming context(s): \" + \", \".join([custom_prefix.sub(\"\", name) for name in norms]))\n        return norms\n\n    url += \"/v2/norming/custom/\"\n    if name and re.search(\"[^a-z0-9_.-]\", name):\n        msg = \"`name` can only include lowercase letters, numbers, hyphens, underscores, or periods\"\n        raise RuntimeError(msg)\n\n    # list current context\n    if verbose:\n        print(\"requesting list of existing custom norming contests\")\n    req = requests.get(url, auth=auth, timeout=9999)\n    if req.status_code != 200:\n        msg = f\"failed to make custom norming list request: {req.status_code} {req.reason}\"\n        raise RuntimeError(msg)\n    norms = pandas.json_normalize(req.json())\n    if len(norms) and \"name\" not in norms:\n        msg = \"`name` column not found in response with columns \" + \", \".join(norms.columns)\n        raise RuntimeError(msg)\n    if not name:\n        if len(norms):\n            if verbose:\n                custom_prefix = re.compile(\"^custom/\")\n                print(\n                    \"custom norming context(s) found: \"\n                    + \", \".join([custom_prefix.sub(\"\", name) for name in norms[\"name\"]])\n                )\n        elif verbose:\n            print(\"no custom norming contexts found\")\n        return norms\n    context_id = \"custom/\" + name\n    if len(norms) and context_id in norms[\"name\"].values:\n        if delete:\n            res = requests.delete(url + name, auth=auth, timeout=9999)\n            content = res.json() if res.text[:1] == \"[\" else {\"message\": res.text}\n            if res.status_code != 200:\n                msg = f\"Request Error ({res.status_code!s})\" + (\n                    (\" (\" + str(content[\"code\"]) + \")\" if \"code\" in content else \"\") + \": \" + content[\"message\"]\n                )\n                raise RuntimeError(msg)\n            return None\n        status = norms[norms[\"name\"] == context_id].iloc[0]\n        if options:\n            warnings.warn(UserWarning(f\"context {name} already exists, so options do not apply\"), stacklevel=2)\n    elif delete:\n        print(f\"context {name} does not exist\")\n        return None\n    else:\n        if verbose:\n            print(f\"requesting creation of context {name}\")\n        req = requests.post(url, json.dumps({\"name\": name, **(options if options else {})}), auth=auth, timeout=9999)\n        if req.status_code != 200:\n            msg = f\"failed to make norming creation request: {req.json().get('error', 'reason unknown')}\"\n            raise RuntimeError(msg)\n        status = pandas.json_normalize(req.json()).iloc[0]\n        if options:\n            for param, value in options.items():\n                if param in status and value != status[param]:\n                    warnings.warn(UserWarning(f\"set option {param} does not match the requested value\"), stacklevel=2)\n    if verbose:\n        print(f\"status of {name}:\")\n        print(status)\n    if text is None:\n        return status\n    status_step = status[\"status\"]\n    if status_step != \"created\":\n        warnings.warn(UserWarning(\"status is not `created`, so cannot send text\"), stacklevel=2)\n        return {\"initial_status\": status, \"first_pass\": None, \"second_pass\": None}\n    if verbose:\n        print(f\"sending first-pass sample for {name}\")\n    _, first_pass, _ = _manage_request(\n        text=text,\n        **kwargs,\n        dotenv=dotenv,\n        key=key,\n        secret=secret,\n        url=f\"{url}{name}/one\",\n        to_norming=True,\n    )\n    second_pass = None\n    if first_pass is not None and (first_pass[\"analyzed_samples\"] == 0).all():\n        warnings.warn(\n            UserWarning(\"no texts were successfully analyzed in the first pass, so second pass was skipped\"),\n            stacklevel=2,\n        )\n    else:\n        if verbose:\n            print(f\"sending second-pass samples for {name}\")\n        _, second_pass, _ = _manage_request(\n            text=text,\n            **kwargs,\n            dotenv=dotenv,\n            key=key,\n            secret=secret,\n            url=f\"{url}{name}/two\",\n            to_norming=True,\n        )\n    if second_pass is None or (second_pass[\"analyzed_samples\"] == 0).all():\n        warnings.warn(UserWarning(\"no texts were successfully analyzed in the second pass\"), stacklevel=2)\n    return {\"initial_stats\": status, \"first_pass\": first_pass, \"second_pass\": second_pass}\n</code></pre>"},{"location":"functions/norming/#receptiviti.norming.norming(name)","title":"<code>name</code>","text":""},{"location":"functions/norming/#receptiviti.norming.norming(text)","title":"<code>text</code>","text":""},{"location":"functions/norming/#receptiviti.norming.norming(options)","title":"<code>options</code>","text":""},{"location":"functions/norming/#receptiviti.norming.norming(delete)","title":"<code>delete</code>","text":""},{"location":"functions/norming/#receptiviti.norming.norming(name_only)","title":"<code>name_only</code>","text":""},{"location":"functions/norming/#receptiviti.norming.norming(dotenv)","title":"<code>dotenv</code>","text":""},{"location":"functions/norming/#receptiviti.norming.norming(key)","title":"<code>key</code>","text":""},{"location":"functions/norming/#receptiviti.norming.norming(secret)","title":"<code>secret</code>","text":""},{"location":"functions/norming/#receptiviti.norming.norming(url)","title":"<code>url</code>","text":""},{"location":"functions/norming/#receptiviti.norming.norming(verbose)","title":"<code>verbose</code>","text":""},{"location":"functions/norming/#receptiviti.norming.norming(**kwargs)","title":"<code>**kwargs</code>","text":""},{"location":"functions/readin_env/","title":"Readin env","text":"<p>Read in a environment variables.</p>"},{"location":"functions/readin_env/#receptiviti.readin_env.readin_env","title":"<code>readin_env(path='.', name='.env', overwrite=False)</code>","text":"<p>Set environment variables from a .env file.</p> PARAMETER DESCRIPTION <p>Path to a .env file, or to a directory containing such a file. By default, this will fall back on <code>~</code> then <code>~/Documents</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'.'</code> </p> <p>Name of the file, when <code>path</code> points to a directory. By default, this will fall back on <code>.Renviron</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'.env'</code> </p> <p>If <code>True</code>, overwrites existing environment variables with the same name as those in the .env file.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> RETURNS DESCRIPTION <code>None</code> <p>If a file is found, it will add contents to <code>os.environ</code>.</p> <p>Examples: <pre><code>receptiviti.readin_env()\n</code></pre></p> Source code in <code>src\\receptiviti\\readin_env.py</code> <pre><code>def readin_env(path: str = \".\", name: str = \".env\", overwrite: bool = False) -&gt; None:\n    \"\"\"\n    Set environment variables from a .env file.\n\n    Args:\n      path (str): Path to a .env file, or to a directory containing such a file.\n        By default, this will fall back on `~` then `~/Documents`.\n      name (str): Name of the file, when `path` points to a directory.\n        By default, this will fall back on `.Renviron`.\n      overwrite (bool): If `True`, overwrites existing environment variables with\n        the same name as those in the .env file.\n\n    Returns:\n      If a file is found, it will add contents to `os.environ`.\n\n    Examples:\n    ```python\n    receptiviti.readin_env()\n    ```\n    \"\"\"\n    path = os.path.expanduser(path)\n    envpath = path if os.path.isfile(path) else path + \"/\" + name\n    if os.path.isfile(envpath):\n        ql = re.compile(\"^['\\\"]|['\\\"\\\\s]+$\")\n        with open(envpath, encoding=\"utf-8\") as file:\n            for line in file:\n                entry = line.split(\"=\", 1)\n                if len(entry) == 2 and (overwrite or os.getenv(entry[0]) is None):\n                    os.environ[entry[0]] = ql.sub(\"\", entry[1])\n    elif name != \".Renviron\":\n        readin_env(path, \".Renviron\", overwrite)\n    elif os.path.isfile(os.path.expanduser(\"~/\") + name):\n        readin_env(\"~\", name, overwrite)\n    elif os.path.isfile(os.path.expanduser(\"~/Documents/\") + name):\n        readin_env(\"~/Documents\", name, overwrite)\n</code></pre>"},{"location":"functions/readin_env/#receptiviti.readin_env.readin_env(path)","title":"<code>path</code>","text":""},{"location":"functions/readin_env/#receptiviti.readin_env.readin_env(name)","title":"<code>name</code>","text":""},{"location":"functions/readin_env/#receptiviti.readin_env.readin_env(overwrite)","title":"<code>overwrite</code>","text":""},{"location":"functions/request/","title":"Request","text":"<p>Make requests to the API.</p>"},{"location":"functions/request/#receptiviti.request.request","title":"<code>request(text=None, output=None, ids=None, text_column=None, id_column=None, files=None, directory=None, file_type='txt', encoding=None, return_text=False, context='written', custom_context=False, api_args=None, frameworks=None, framework_prefix=None, bundle_size=1000, bundle_byte_limit=7500000.0, collapse_lines=False, retry_limit=50, clear_cache=False, request_cache=True, cores=1, collect_results=True, in_memory=None, verbose=False, progress_bar=os.getenv('RECEPTIVITI_PB', 'True'), overwrite=False, make_request=True, text_as_paths=False, dotenv=True, cache=os.getenv('RECEPTIVITI_CACHE', ''), cache_degragment=True, cache_overwrite=False, cache_format=os.getenv('RECEPTIVITI_CACHE_FORMAT', ''), key=os.getenv('RECEPTIVITI_KEY', ''), secret=os.getenv('RECEPTIVITI_SECRET', ''), url=os.getenv('RECEPTIVITI_URL', ''), version=os.getenv('RECEPTIVITI_VERSION', ''), endpoint=os.getenv('RECEPTIVITI_ENDPOINT', ''))</code>","text":"<p>Send texts to be scored by the API.</p> PARAMETER DESCRIPTION <p>Text to be processed, as a string or vector of strings containing the text itself, or the path to a file from which to read in text. If a DataFrame, <code>text_column</code> is used to extract such a vector. A string may also represent a directory in which to search for files. To best ensure paths are not treated as texts, either set <code>text_as_path</code> to <code>True</code>, or use <code>directory</code> to enter a directory path, or <code>files</code> to enter a vector of file paths.</p> <p> TYPE: <code>str | list[str] | DataFrame</code> DEFAULT: <code>None</code> </p> <p>Path to a file to write results to.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <p>Vector of IDs for each <code>text</code>, or a column name in <code>text</code> containing IDs.</p> <p> TYPE: <code>str | list[str | int]</code> DEFAULT: <code>None</code> </p> <p>Column name in <code>text</code> containing text.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <p>Column name in <code>text</code> containing IDs.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <p>Vector of file paths, as alternate entry to <code>text</code>.</p> <p> TYPE: <code>list[str]</code> DEFAULT: <code>None</code> </p> <p>A directory path to search for files in, as alternate entry to <code>text</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>None</code> </p> <p>Extension of the file(s) to be read in from a directory (<code>txt</code> or <code>csv</code>).</p> <p> TYPE: <code>str</code> DEFAULT: <code>'txt'</code> </p> <p>Encoding of file(s) to be read in; one of the standard encodings. If this is <code>None</code> (default), encoding will be predicted for each file, but this can potentially fail, resulting in mis-encoded characters. For best (and fastest) results, specify encoding.</p> <p> TYPE: <code>str | None</code> DEFAULT: <code>None</code> </p> <p>If <code>True</code>, will include a <code>text</code> column in the output with the original text.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Name of the analysis context.</p> <p> TYPE: <code>str</code> DEFAULT: <code>'written'</code> </p> <p>Name of a custom context (as listed by <code>receptiviti.norming</code>), or <code>True</code> if <code>context</code> is the name of a custom context.</p> <p> TYPE: <code>str | bool</code> DEFAULT: <code>False</code> </p> <p>Additional arguments to include in the request.</p> <p> TYPE: <code>dict</code> DEFAULT: <code>None</code> </p> <p>One or more names of frameworks to request. Note that this changes the results from the API, so it will invalidate any cached results without the same set of frameworks.</p> <p> TYPE: <code>str | list</code> DEFAULT: <code>None</code> </p> <p>If <code>False</code>, will drop framework prefix from column names. If one framework is selected, will default to <code>False</code>.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>None</code> </p> <p>Maximum number of texts per bundle.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <p>Maximum byte size of each bundle.</p> <p> TYPE: <code>float</code> DEFAULT: <code>7500000.0</code> </p> <p>If <code>True</code>, will treat files as containing single texts, and collapse multiple lines.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Number of times to retry a failed request.</p> <p> TYPE: <code>int</code> DEFAULT: <code>50</code> </p> <p>If <code>True</code>, will delete the <code>cache</code> before processing.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>If <code>False</code>, will not temporarily save raw requests for reuse within a day.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <p>Number of CPU cores to use when processing multiple bundles.</p> <p> TYPE: <code>int</code> DEFAULT: <code>1</code> </p> <p>If <code>False</code>, will not retain bundle results in memory for return.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <p>If <code>False</code>, will write bundles to disc, to be loaded when processed. Defaults to <code>True</code> when processing in parallel.</p> <p> TYPE: <code>bool | None</code> DEFAULT: <code>None</code> </p> <p>If <code>True</code>, will print status messages and preserve the progress bar.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>If <code>False</code>, will not display a progress bar.</p> <p> TYPE: <code>str | bool</code> DEFAULT: <code>getenv('RECEPTIVITI_PB', 'True')</code> </p> <p>If <code>True</code>, will overwrite an existing <code>output</code> file.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>If <code>True</code>, will explicitly mark <code>text</code> as a list of file paths. Otherwise, this will be detected.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>Path to a .env file to read environment variables from. By default, will for a file in the current directory or <code>~/Documents</code>. Passed to <code>readin_env</code> as <code>path</code>.</p> <p> TYPE: <code>bool | str</code> DEFAULT: <code>True</code> </p> <p>Path to a cache directory, or <code>True</code> to use the default directory. The cache is an Arrow dataset, and so requires the <code>pyarrow</code> package.</p> <p> TYPE: <code>bool | str</code> DEFAULT: <code>getenv('RECEPTIVITI_CACHE', '')</code> </p> <p>If <code>False</code>, will not defragment the cache after writing new results to it.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <p>If <code>True</code>, will not check the cache for previously cached texts, but will store results in the cache (unlike <code>cache = False</code>).</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <p>File format of the cache, of available Arrow formats.</p> <p> TYPE: <code>str</code> DEFAULT: <code>getenv('RECEPTIVITI_CACHE_FORMAT', '')</code> </p> <p>Your API key.</p> <p> TYPE: <code>str</code> DEFAULT: <code>getenv('RECEPTIVITI_KEY', '')</code> </p> <p>Your API secret.</p> <p> TYPE: <code>str</code> DEFAULT: <code>getenv('RECEPTIVITI_SECRET', '')</code> </p> <p>The URL of the API; defaults to <code>https://api.receptiviti.com</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>getenv('RECEPTIVITI_URL', '')</code> </p> <p>Version of the API; defaults to <code>v1</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>getenv('RECEPTIVITI_VERSION', '')</code> </p> <p>Endpoint of the API; defaults to <code>framework</code>.</p> <p> TYPE: <code>str</code> DEFAULT: <code>getenv('RECEPTIVITI_ENDPOINT', '')</code> </p> RETURNS DESCRIPTION <code>Union[DataFrame, None]</code> <p>Scores associated with each input text.</p> <p>Examples:</p> <pre><code># score a single text\nsingle = receptiviti.request(\"a text to score\")\n\n# score multiple texts, and write results to a file\nmulti = receptiviti.request([\"first text to score\", \"second text\"], \"filename.csv\")\n\n# score texts in separate files\n## defaults to look for .txt files\nfile_results = receptiviti.request(directory = \"./path/to/txt_folder\")\n\n## could be .csv\nfile_results = receptiviti.request(\n    directory = \"./path/to/csv_folder\",\n    text_column = \"text\", file_type = \"csv\"\n)\n\n# score texts in a single file\nresults = receptiviti.request(\"./path/to/file.csv\", text_column = \"text\")\n</code></pre> Request Process <p>This function (along with the internal <code>_manage_request</code> function) handles texts and results in several steps:</p> <ol> <li>Prepare bundles (split <code>text</code> into &lt;= <code>bundle_size</code> and &lt;= <code>bundle_byte_limit</code> bundles).<ol> <li>If <code>text</code> points to a directory or list of files, these will be read in later.</li> <li>If <code>in_memory</code> is <code>False</code>, bundles are written to a temporary location,    and read back in when the request is made.</li> </ol> </li> <li>Get scores for texts within each bundle.<ol> <li>If texts are paths, or <code>in_memory</code> is <code>False</code>, will load texts.</li> <li>If <code>cache</code> is set, will skip any texts with cached scores.</li> <li>If <code>request_cache</code> is <code>True</code>, will check for a cached request.</li> <li>If any texts need scoring and <code>make_request</code> is <code>True</code>, will send unscored texts to the API.</li> </ol> </li> <li>If a request was made and <code>request_cache</code> is set, will cache the response.</li> <li>If <code>cache</code> is set, will write bundle scores to the cache.</li> <li>After requests are made, if <code>cache</code> is set, will defragment the cache    (combine bundle results within partitions).</li> <li>If <code>collect_results</code> is <code>True</code>, will prepare results:<ol> <li>Will realign results with <code>text</code> (and <code>ids</code> if provided).</li> <li>If <code>output</code> is specified, will write realigned results to it.</li> <li>Will drop additional columns (such as <code>custom</code> and <code>id</code> if not provided).</li> <li>If <code>framework</code> is specified, will use it to select columns of the results.</li> <li>Returns results.</li> </ol> </li> </ol> Cache <p>If <code>cache</code> is specified, results for unique texts are saved in an Arrow database in the cache location (<code>os.getenv(\"RECEPTIVITI_CACHE\")</code>), and are retrieved with subsequent requests. This ensures that the exact same texts are not re-sent to the API. This does, however, add some processing time and disc space usage.</p> <p>If <code>cache</code> if <code>True</code>, a default directory (<code>receptiviti_cache</code>) will be looked for in the system's temporary directory (<code>tempfile.gettempdir()</code>).</p> <p>The primary cache is checked when each bundle is processed, and existing results are loaded at that time. When processing many bundles in parallel, and many results have been cached, this can cause the system to freeze and potentially crash. To avoid this, limit the number of cores, or disable parallel processing.</p> <p>The <code>cache_format</code> arguments (or the <code>RECEPTIVITI_CACHE_FORMAT</code> environment variable) can be used to adjust the format of the cache.</p> <p>You can use the cache independently with <code>pyarrow.dataset.dataset(os.getenv(\"RECEPTIVITI_CACHE\"))</code>.</p> <p>You can also set the <code>clear_cache</code> argument to <code>True</code> to clear the cache before it is used again, which may be useful if the cache has gotten big, or you know new results will be returned.</p> <p>Even if a cached result exists, it will be reprocessed if it does not have all of the variables of new results, but this depends on there being at least 1 uncached result. If, for instance, you add a framework to your account and want to reprocess a previously processed set of texts, you would need to first clear the cache.</p> <p>Either way, duplicated texts within the same call will only be sent once.</p> <p>The <code>request_cache</code> argument controls a more temporary cache of each bundle request. This is cleared after a day. You might want to set this to <code>False</code> if a new framework becomes available on your account and you want to process a set of text you re-processed recently.</p> <p>Another temporary cache is made when <code>in_memory</code> is <code>False</code>, which is the default when processing in parallel (when there is more than 1 bundle and <code>cores</code> is over 1). This is a temporary directory that contains a file for each unique bundle, which is read in as needed by the parallel workers.</p> Parallelization <p><code>text</code>s are split into bundles based on the <code>bundle_size</code> argument. Each bundle represents a single request to the API, which is why they are limited to 1000 texts and a total size of 10 MB. When there is more than one bundle and <code>cores</code> is greater than 1, bundles are processed by multiple cores.</p> <p>If you have texts spread across multiple files, they can be most efficiently processed in parallel if each file contains a single text (potentially collapsed from multiple lines). If files contain multiple texts (i.e., <code>collapse_lines=False</code>), then texts need to be read in before bundling in order to ensure bundles are under the length limit.</p> <p>If you are calling this function from a script, parallelization will involve rerunning that script in each process, so anything you don't want rerun should be protected by a check that <code>__name__</code> equals <code>\"__main__\"</code> (placed within an <code>if __name__ == \"__main__\":</code> clause).</p> Source code in <code>src\\receptiviti\\request.py</code> <pre><code>def request(\n    text: Union[str, List[str], pandas.DataFrame, None] = None,\n    output: Union[str, None] = None,\n    ids: Union[str, List[str], List[int], None] = None,\n    text_column: Union[str, None] = None,\n    id_column: Union[str, None] = None,\n    files: Union[List[str], None] = None,\n    directory: Union[str, None] = None,\n    file_type: str = \"txt\",\n    encoding: Union[str, None] = None,\n    return_text: bool = False,\n    context: str = \"written\",\n    custom_context: Union[str, bool] = False,\n    api_args: Union[Dict[str, Union[str, List[str]]], None] = None,\n    frameworks: Union[str, List[str], None] = None,\n    framework_prefix: Union[bool, None] = None,\n    bundle_size: int = 1000,\n    bundle_byte_limit: float = 75e5,\n    collapse_lines: bool = False,\n    retry_limit: int = 50,\n    clear_cache: bool = False,\n    request_cache: bool = True,\n    cores: int = 1,\n    collect_results: bool = True,\n    in_memory: Union[bool, None] = None,\n    verbose: bool = False,\n    progress_bar: Union[str, bool] = os.getenv(\"RECEPTIVITI_PB\", \"True\"),\n    overwrite: bool = False,\n    make_request: bool = True,\n    text_as_paths: bool = False,\n    dotenv: Union[bool, str] = True,\n    cache: Union[str, bool] = os.getenv(\"RECEPTIVITI_CACHE\", \"\"),\n    cache_degragment: bool = True,\n    cache_overwrite: bool = False,\n    cache_format: str = os.getenv(\"RECEPTIVITI_CACHE_FORMAT\", \"\"),\n    key: str = os.getenv(\"RECEPTIVITI_KEY\", \"\"),\n    secret: str = os.getenv(\"RECEPTIVITI_SECRET\", \"\"),\n    url: str = os.getenv(\"RECEPTIVITI_URL\", \"\"),\n    version: str = os.getenv(\"RECEPTIVITI_VERSION\", \"\"),\n    endpoint: str = os.getenv(\"RECEPTIVITI_ENDPOINT\", \"\"),\n) -&gt; Union[pandas.DataFrame, None]:\n    \"\"\"\n    Send texts to be scored by the API.\n\n    Args:\n        text (str | list[str] | pandas.DataFrame): Text to be processed, as a string or vector of\n            strings containing the text itself, or the path to a file from which to read in text.\n            If a DataFrame, `text_column` is used to extract such a vector. A string may also\n            represent a directory in which to search for files. To best ensure paths are not\n            treated as texts, either set `text_as_path` to `True`, or use `directory` to enter\n            a directory path, or `files` to enter a vector of file paths.\n        output (str): Path to a file to write results to.\n        ids (str | list[str | int]): Vector of IDs for each `text`, or a column name in `text`\n            containing IDs.\n        text_column (str): Column name in `text` containing text.\n        id_column (str): Column name in `text` containing IDs.\n        files (list[str]): Vector of file paths, as alternate entry to `text`.\n        directory (str): A directory path to search for files in, as alternate entry to `text`.\n        file_type (str): Extension of the file(s) to be read in from a directory (`txt` or `csv`).\n        encoding (str | None): Encoding of file(s) to be read in; one of the\n            [standard encodings](https://docs.python.org/3/library/codecs.html#standard-encodings).\n            If this is `None` (default), encoding will be predicted for each file, but this can\n            potentially fail, resulting in mis-encoded characters. For best (and fastest) results,\n            specify encoding.\n        return_text (bool): If `True`, will include a `text` column in the output with the\n            original text.\n        context (str): Name of the analysis context.\n        custom_context (str | bool): Name of a custom context (as listed by `receptiviti.norming`),\n            or `True` if `context` is the name of a custom context.\n        api_args (dict): Additional arguments to include in the request.\n        frameworks (str | list): One or more names of frameworks to request. Note that this\n            changes the results from the API, so it will invalidate any cached results\n            without the same set of frameworks.\n        framework_prefix (bool): If `False`, will drop framework prefix from column names.\n            If one framework is selected, will default to `False`.\n        bundle_size (int): Maximum number of texts per bundle.\n        bundle_byte_limit (float): Maximum byte size of each bundle.\n        collapse_lines (bool): If `True`, will treat files as containing single texts, and\n            collapse multiple lines.\n        retry_limit (int): Number of times to retry a failed request.\n        clear_cache (bool): If `True`, will delete the `cache` before processing.\n        request_cache (bool): If `False`, will not temporarily save raw requests for reuse\n            within a day.\n        cores (int): Number of CPU cores to use when processing multiple bundles.\n        collect_results (bool): If `False`, will not retain bundle results in memory for return.\n        in_memory (bool | None): If `False`, will write bundles to disc, to be loaded when\n            processed. Defaults to `True` when processing in parallel.\n        verbose (bool): If `True`, will print status messages and preserve the progress bar.\n        progress_bar (str | bool): If `False`, will not display a progress bar.\n        overwrite (bool): If `True`, will overwrite an existing `output` file.\n        text_as_paths (bool): If `True`, will explicitly mark `text` as a list of file paths.\n            Otherwise, this will be detected.\n        dotenv (bool | str): Path to a .env file to read environment variables from. By default,\n            will for a file in the current directory or `~/Documents`.\n            Passed to `readin_env` as `path`.\n        cache (bool | str): Path to a cache directory, or `True` to use the default directory.\n            The cache is an Arrow dataset, and so requires the `pyarrow` package.\n        cache_degragment (bool): If `False`, will not defragment the cache after writing new\n            results to it.\n        cache_overwrite (bool): If `True`, will not check the cache for previously cached texts,\n            but will store results in the cache (unlike `cache = False`).\n        cache_format (str): File format of the cache, of available Arrow formats.\n        key (str): Your API key.\n        secret (str): Your API secret.\n        url (str): The URL of the API; defaults to `https://api.receptiviti.com`.\n        version (str): Version of the API; defaults to `v1`.\n        endpoint (str): Endpoint of the API; defaults to `framework`.\n\n    Returns:\n        Scores associated with each input text.\n\n    Examples:\n        ```python\n        # score a single text\n        single = receptiviti.request(\"a text to score\")\n\n        # score multiple texts, and write results to a file\n        multi = receptiviti.request([\"first text to score\", \"second text\"], \"filename.csv\")\n\n        # score texts in separate files\n        ## defaults to look for .txt files\n        file_results = receptiviti.request(directory = \"./path/to/txt_folder\")\n\n        ## could be .csv\n        file_results = receptiviti.request(\n            directory = \"./path/to/csv_folder\",\n            text_column = \"text\", file_type = \"csv\"\n        )\n\n        # score texts in a single file\n        results = receptiviti.request(\"./path/to/file.csv\", text_column = \"text\")\n        ```\n\n    Request Process:\n        This function (along with the internal `_manage_request` function) handles texts and results in several steps:\n\n        1. Prepare bundles (split `text` into &lt;= `bundle_size` and &lt;= `bundle_byte_limit` bundles).\n            1. If `text` points to a directory or list of files, these will be read in later.\n            2. If `in_memory` is `False`, bundles are written to a temporary location,\n               and read back in when the request is made.\n        2. Get scores for texts within each bundle.\n            1. If texts are paths, or `in_memory` is `False`, will load texts.\n            2. If `cache` is set, will skip any texts with cached scores.\n            3. If `request_cache` is `True`, will check for a cached request.\n            4. If any texts need scoring and `make_request` is `True`, will send unscored texts to the API.\n        3. If a request was made and `request_cache` is set, will cache the response.\n        4. If `cache` is set, will write bundle scores to the cache.\n        5. After requests are made, if `cache` is set, will defragment the cache\n           (combine bundle results within partitions).\n        6. If `collect_results` is `True`, will prepare results:\n            1. Will realign results with `text` (and `ids` if provided).\n            2. If `output` is specified, will write realigned results to it.\n            3. Will drop additional columns (such as `custom` and `id` if not provided).\n            4. If `framework` is specified, will use it to select columns of the results.\n            5. Returns results.\n\n    Cache:\n        If `cache` is specified, results for unique texts are saved in an Arrow database\n        in the cache location (`os.getenv(\"RECEPTIVITI_CACHE\")`), and are retrieved with\n        subsequent requests. This ensures that the exact same texts are not re-sent to the API.\n        This does, however, add some processing time and disc space usage.\n\n        If `cache` if `True`, a default directory (`receptiviti_cache`) will be\n        looked for in the system's temporary directory (`tempfile.gettempdir()`).\n\n        The primary cache is checked when each bundle is processed, and existing results are\n        loaded at that time. When processing many bundles in parallel, and many results have\n        been cached, this can cause the system to freeze and potentially crash.\n        To avoid this, limit the number of cores, or disable parallel processing.\n\n        The `cache_format` arguments (or the `RECEPTIVITI_CACHE_FORMAT` environment variable) can be\n        used to adjust the format of the cache.\n\n        You can use the cache independently with\n        `pyarrow.dataset.dataset(os.getenv(\"RECEPTIVITI_CACHE\"))`.\n\n        You can also set the `clear_cache` argument to `True` to clear the cache before it is used\n        again, which may be useful if the cache has gotten big, or you know new results will be\n        returned.\n\n        Even if a cached result exists, it will be reprocessed if it does not have all of the\n        variables of new results, but this depends on there being at least 1 uncached result. If,\n        for instance, you add a framework to your account and want to reprocess a previously\n        processed set of texts, you would need to first clear the cache.\n\n        Either way, duplicated texts within the same call will only be sent once.\n\n        The `request_cache` argument controls a more temporary cache of each bundle request. This\n        is cleared after a day. You might want to set this to `False` if a new framework becomes\n        available on your account and you want to process a set of text you re-processed recently.\n\n        Another temporary cache is made when `in_memory` is `False`, which is the default when\n        processing in parallel (when there is more than 1 bundle and `cores` is over 1). This is a\n        temporary directory that contains a file for each unique bundle, which is read in as needed\n        by the parallel workers.\n\n    Parallelization:\n        `text`s are split into bundles based on the `bundle_size` argument. Each bundle represents\n        a single request to the API, which is why they are limited to 1000 texts and a total size\n        of 10 MB. When there is more than one bundle and `cores` is greater than 1, bundles are\n        processed by multiple cores.\n\n        If you have texts spread across multiple files, they can be most efficiently processed in\n        parallel if each file contains a single text (potentially collapsed from multiple lines).\n        If files contain multiple texts (i.e., `collapse_lines=False`), then texts need to be\n        read in before bundling in order to ensure bundles are under the length limit.\n\n        If you are calling this function from a script, parallelization will involve rerunning\n        that script in each process, so anything you don't want rerun should be protected by\n        a check that `__name__` equals `\"__main__\"`\n        (placed within an `if __name__ == \"__main__\":` clause).\n    \"\"\"\n    if cores &gt; 1 and current_process().name != \"MainProcess\":\n        return None\n    if output is not None and os.path.isfile(output) and not overwrite:\n        msg = \"`output` file already exists; use `overwrite=True` to overwrite it\"\n        raise RuntimeError(msg)\n    start_time = perf_counter()\n\n    if dotenv:\n        readin_env(dotenv if isinstance(dotenv, str) else \".\")\n        dotenv = False\n\n    # check norming context\n    if isinstance(custom_context, str):\n        context = custom_context\n        custom_context = True\n    if context != \"written\":\n        if verbose:\n            print(f\"retrieving norming contexts ({perf_counter() - start_time:.4f})\")\n        available_contexts = norming(name_only=True, url=url, key=key, secret=secret, verbose=False)\n        if (\n            not isinstance(available_contexts, list)\n            or (\"custom/\" + context if custom_context else context) not in available_contexts\n        ):\n            msg = f\"norming context {context} is not on record or is not completed\"\n            raise RuntimeError(msg)\n\n    # check frameworks\n    if frameworks and version and \"2\" in version:\n        if not api_args:\n            api_args = {}\n        if isinstance(frameworks, str):\n            frameworks = [frameworks]\n        api_args[\"frameworks\"] = [f for f in frameworks if f != \"summary\"]\n    if api_args and \"frameworks\" in api_args:\n        arg_frameworks: List[str] = (\n            api_args[\"frameworks\"].split(\",\") if isinstance(api_args[\"frameworks\"], str) else api_args[\"frameworks\"]\n        )\n        available_frameworks = get_frameworks(url=url, key=key, secret=secret)\n        for f in arg_frameworks:\n            if f not in available_frameworks:\n                msg = f\"requested framework is not available to your account: {f}\"\n                raise RuntimeError(msg)\n        if isinstance(api_args[\"frameworks\"], list):\n            api_args[\"frameworks\"] = \",\".join(api_args[\"frameworks\"])\n\n    if isinstance(cache, str) and cache:\n        if find_spec(\"pyarrow\") is None:\n            msg = \"install the `pyarrow` package to use the cache\"\n            raise RuntimeError(msg)\n        if clear_cache and os.path.exists(cache):\n            shutil.rmtree(cache, True)\n        os.makedirs(cache, exist_ok=True)\n        if not cache_format:\n            cache_format = os.getenv(\"RECEPTIVITI_CACHE_FORMAT\", \"parquet\")\n        if cache_format not in [\"parquet\", \"feather\"]:\n            msg = \"`cache_format` must be `parquet` or `feather`\"\n            raise RuntimeError(msg)\n    else:\n        cache = \"\"\n\n    data, res, id_specified = _manage_request(\n        text=text,\n        ids=ids,\n        text_column=text_column,\n        id_column=id_column,\n        files=files,\n        directory=directory,\n        file_type=file_type,\n        encoding=encoding,\n        context=f\"custom/{context}\" if custom_context else context,\n        api_args=api_args,\n        bundle_size=bundle_size,\n        bundle_byte_limit=bundle_byte_limit,\n        collapse_lines=collapse_lines,\n        retry_limit=retry_limit,\n        request_cache=request_cache,\n        cores=cores,\n        collect_results=collect_results,\n        in_memory=in_memory,\n        verbose=verbose,\n        progress_bar=progress_bar,\n        make_request=make_request,\n        text_as_paths=text_as_paths,\n        dotenv=dotenv,\n        cache=cache,\n        cache_overwrite=cache_overwrite,\n        cache_format=cache_format,\n        key=key,\n        secret=secret,\n        url=url,\n        version=version,\n        endpoint=endpoint,\n    )\n\n    # finalize\n    if collect_results and (res is None or not res.shape[0]):\n        msg = \"no results\"\n        raise RuntimeError(msg)\n    if cache and cache_degragment:\n        writer = _get_writer(cache_format)\n        for bin_dir in glob(cache + \"/bin=*/\"):\n            _defragment_bin(bin_dir, \"feather\" if cache_format == \"feather\" else \"parquet\", writer)\n    if not collect_results or res is None:\n        if verbose:\n            print(f\"done ({perf_counter() - start_time:.4f})\")\n        return None\n    if verbose:\n        print(f\"preparing output ({perf_counter() - start_time:.4f})\")\n    data.set_index(\"id\", inplace=True)\n    res.set_index(\"id\", inplace=True)\n    if len(res) != len(data):\n        res = res.join(data[\"text\"])\n        data_absent = data.loc[list(set(data.index).difference(res.index))]\n        data_absent = data_absent.loc[data_absent[\"text\"].isin(res[\"text\"])]\n        if data.size:\n            res = res.reset_index()\n            res.set_index(\"text\", inplace=True)\n            data_dupes = res.loc[data_absent[\"text\"]]\n            data_dupes[\"id\"] = data_absent.index.to_list()\n            res = pandas.concat([res, data_dupes])\n            res.reset_index(inplace=True, drop=True)\n            res.set_index(\"id\", inplace=True)\n    res = res.join(data[\"text\"], how=\"right\")\n    if not return_text:\n        res.drop(\"text\", axis=1, inplace=True)\n    res = res.reset_index()\n\n    if output is not None:\n        if verbose:\n            print(f\"writing results to file: {output} ({perf_counter() - start_time:.4f})\")\n        res.to_csv(output, index=False)\n\n    drops = [\"custom\", \"bin\"]\n    if not id_specified:\n        drops.append(\"id\")\n    res.drop(\n        list({*drops}.intersection(res.columns)),\n        axis=\"columns\",\n        inplace=True,\n    )\n    if frameworks is not None:\n        if verbose:\n            print(f\"selecting frameworks ({perf_counter() - start_time:.4f})\")\n        if isinstance(frameworks, str):\n            frameworks = [frameworks]\n        if len(frameworks) == 1 and framework_prefix is None:\n            framework_prefix = False\n        select = []\n        if id_specified:\n            select.append(\"id\")\n        if return_text:\n            select.append(\"text\")\n        select.append(\"text_hash\")\n        res = res.filter(regex=f\"^(?:{'|'.join(select + frameworks)})(?:$|\\\\.)\")\n    if isinstance(framework_prefix, bool) and not framework_prefix:\n        prefix_pattern = re.compile(\"^[^.]+\\\\.\")\n        res.columns = pandas.Index([prefix_pattern.sub(\"\", col) for col in res.columns])\n\n    if verbose:\n        print(f\"done ({perf_counter() - start_time:.4f})\")\n\n    return res\n</code></pre>"},{"location":"functions/request/#receptiviti.request.request(text)","title":"<code>text</code>","text":""},{"location":"functions/request/#receptiviti.request.request(output)","title":"<code>output</code>","text":""},{"location":"functions/request/#receptiviti.request.request(ids)","title":"<code>ids</code>","text":""},{"location":"functions/request/#receptiviti.request.request(text_column)","title":"<code>text_column</code>","text":""},{"location":"functions/request/#receptiviti.request.request(id_column)","title":"<code>id_column</code>","text":""},{"location":"functions/request/#receptiviti.request.request(files)","title":"<code>files</code>","text":""},{"location":"functions/request/#receptiviti.request.request(directory)","title":"<code>directory</code>","text":""},{"location":"functions/request/#receptiviti.request.request(file_type)","title":"<code>file_type</code>","text":""},{"location":"functions/request/#receptiviti.request.request(encoding)","title":"<code>encoding</code>","text":""},{"location":"functions/request/#receptiviti.request.request(return_text)","title":"<code>return_text</code>","text":""},{"location":"functions/request/#receptiviti.request.request(context)","title":"<code>context</code>","text":""},{"location":"functions/request/#receptiviti.request.request(custom_context)","title":"<code>custom_context</code>","text":""},{"location":"functions/request/#receptiviti.request.request(api_args)","title":"<code>api_args</code>","text":""},{"location":"functions/request/#receptiviti.request.request(frameworks)","title":"<code>frameworks</code>","text":""},{"location":"functions/request/#receptiviti.request.request(framework_prefix)","title":"<code>framework_prefix</code>","text":""},{"location":"functions/request/#receptiviti.request.request(bundle_size)","title":"<code>bundle_size</code>","text":""},{"location":"functions/request/#receptiviti.request.request(bundle_byte_limit)","title":"<code>bundle_byte_limit</code>","text":""},{"location":"functions/request/#receptiviti.request.request(collapse_lines)","title":"<code>collapse_lines</code>","text":""},{"location":"functions/request/#receptiviti.request.request(retry_limit)","title":"<code>retry_limit</code>","text":""},{"location":"functions/request/#receptiviti.request.request(clear_cache)","title":"<code>clear_cache</code>","text":""},{"location":"functions/request/#receptiviti.request.request(request_cache)","title":"<code>request_cache</code>","text":""},{"location":"functions/request/#receptiviti.request.request(cores)","title":"<code>cores</code>","text":""},{"location":"functions/request/#receptiviti.request.request(collect_results)","title":"<code>collect_results</code>","text":""},{"location":"functions/request/#receptiviti.request.request(in_memory)","title":"<code>in_memory</code>","text":""},{"location":"functions/request/#receptiviti.request.request(verbose)","title":"<code>verbose</code>","text":""},{"location":"functions/request/#receptiviti.request.request(progress_bar)","title":"<code>progress_bar</code>","text":""},{"location":"functions/request/#receptiviti.request.request(overwrite)","title":"<code>overwrite</code>","text":""},{"location":"functions/request/#receptiviti.request.request(text_as_paths)","title":"<code>text_as_paths</code>","text":""},{"location":"functions/request/#receptiviti.request.request(dotenv)","title":"<code>dotenv</code>","text":""},{"location":"functions/request/#receptiviti.request.request(cache)","title":"<code>cache</code>","text":""},{"location":"functions/request/#receptiviti.request.request(cache_degragment)","title":"<code>cache_degragment</code>","text":""},{"location":"functions/request/#receptiviti.request.request(cache_overwrite)","title":"<code>cache_overwrite</code>","text":""},{"location":"functions/request/#receptiviti.request.request(cache_format)","title":"<code>cache_format</code>","text":""},{"location":"functions/request/#receptiviti.request.request(key)","title":"<code>key</code>","text":""},{"location":"functions/request/#receptiviti.request.request(secret)","title":"<code>secret</code>","text":""},{"location":"functions/request/#receptiviti.request.request(url)","title":"<code>url</code>","text":""},{"location":"functions/request/#receptiviti.request.request(version)","title":"<code>version</code>","text":""},{"location":"functions/request/#receptiviti.request.request(endpoint)","title":"<code>endpoint</code>","text":""},{"location":"functions/status/","title":"Status","text":"<p>Check the status of the API.</p>"},{"location":"functions/status/#receptiviti.status.status","title":"<code>status(url=os.getenv('RECEPTIVITI_URL', ''), key=os.getenv('RECEPTIVITI_KEY', ''), secret=os.getenv('RECEPTIVITI_SECRET', ''), dotenv=False, verbose=True)</code>","text":"<p>Check the API's status.</p> <p>Ping the Receptiviti API to see if it's available, and if your credentials are valid.</p> PARAMETER DESCRIPTION <p>The URL of the API.</p> <p> TYPE: <code>str</code> DEFAULT: <code>getenv('RECEPTIVITI_URL', '')</code> </p> <p>Your API key.</p> <p> TYPE: <code>str</code> DEFAULT: <code>getenv('RECEPTIVITI_KEY', '')</code> </p> <p>Your API secret.</p> <p> TYPE: <code>str</code> DEFAULT: <code>getenv('RECEPTIVITI_SECRET', '')</code> </p> <p>Path to a .env file to read environment variables from, or <code>True</code> to look for a file in the current directory.</p> <p> TYPE: <code>bool | str</code> DEFAULT: <code>False</code> </p> <p>If <code>False</code>, will not print status messages.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RETURNS DESCRIPTION <code>Union[Response, None]</code> <p>Response from the API server.</p> <p>Examples:</p> <pre><code>receptiviti.status()\n</code></pre> Source code in <code>src\\receptiviti\\status.py</code> <pre><code>def status(\n    url: str = os.getenv(\"RECEPTIVITI_URL\", \"\"),\n    key: str = os.getenv(\"RECEPTIVITI_KEY\", \"\"),\n    secret: str = os.getenv(\"RECEPTIVITI_SECRET\", \"\"),\n    dotenv: Union[bool, str] = False,\n    verbose: bool = True,\n) -&gt; Union[requests.Response, None]:\n    \"\"\"\n    Check the API's status.\n\n    Ping the Receptiviti API to see if it's available, and if your credentials are valid.\n\n    Args:\n      url (str): The URL of the API.\n      key (str): Your API key.\n      secret (str): Your API secret.\n      dotenv (bool | str): Path to a .env file to read environment variables from, or `True`\n        to look for a file in the current directory.\n      verbose (bool): If `False`, will not print status messages.\n\n    Returns:\n      Response from the API server.\n\n    Examples:\n        ```python\n        receptiviti.status()\n        ```\n    \"\"\"\n    _, url, key, secret = _resolve_request_def(url, key, secret, dotenv)\n    try:\n        res = requests.get(url.lower() + \"/v1/ping\", auth=(key, secret), timeout=9999)\n    except requests.exceptions.RequestException:\n        if verbose:\n            print(\"Status: ERROR\\nMessage: URL is unreachable\")\n        return None\n    content = res.json() if res.text[:1] == \"{\" else {\"message\": res.text}\n    if verbose:\n        print(\"Status: \" + (\"OK\" if res.status_code == 200 else \"ERROR\"))\n        print(\n            \"Message: \"\n            + (\n                str(res.status_code)\n                + (\" (\" + str(content[\"code\"]) + \")\" if \"code\" in content else \"\")\n                + \": \"\n                + content[\"pong\" if \"pong\" in content else \"message\"]\n            )\n        )\n    return res\n</code></pre>"},{"location":"functions/status/#receptiviti.status.status(url)","title":"<code>url</code>","text":""},{"location":"functions/status/#receptiviti.status.status(key)","title":"<code>key</code>","text":""},{"location":"functions/status/#receptiviti.status.status(secret)","title":"<code>secret</code>","text":""},{"location":"functions/status/#receptiviti.status.status(dotenv)","title":"<code>dotenv</code>","text":""},{"location":"functions/status/#receptiviti.status.status(verbose)","title":"<code>verbose</code>","text":""}]}